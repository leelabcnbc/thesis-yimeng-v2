{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "from os import makedirs\n",
    "from os.path import relpath, realpath, abspath, join, exists, dirname\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "pd.set_option('display.max_rows', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnetjson.builder import build_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_v2 import dir_dict\n",
    "from thesis_v2.data import load_data_helper\n",
    "from thesis_v2.training.training_aux import load_training_results\n",
    "from thesis_v2.training_extra.misc import count_params\n",
    "from thesis_v2.models.maskcnn_polished_with_local_pcn.builder import load_modules\n",
    "\n",
    "load_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_check = 'scripts/training/yuanyuan_8k_a_3day/maskcnn_polished_with_local_pcn'\n",
    "path.insert(0, join(dir_dict['root'], folder_to_check))\n",
    "from submit_certain_configs import param_iterator_obj\n",
    "from key_utils import keygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_v2.training_extra.data import generate_datasets\n",
    "from thesis_v2.training_extra.evaluation import eval_fn_wrapper as eval_fn_wrapper_neural\n",
    "from thesis_v2.training.training import eval_wrapper\n",
    "from thesis_v2.data.prepared.yuanyuan_8k import get_data, get_neural_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ccmax\n",
    "from strflab.stats import cc_max\n",
    "from thesis_v2.data.prepared.yuanyuan_8k import get_neural_data_per_trial\n",
    "\n",
    "cc_max_all_neurons = cc_max(get_neural_data_per_trial(\n",
    "    ('042318', '043018','051018',))\n",
    "                           )\n",
    "assert cc_max_all_neurons.shape == (79,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.backends import cudnn\n",
    "import torch\n",
    "cudnn.enabled = True\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "\n",
    "_data_cache = dict()\n",
    "\n",
    "def get_data_cached(input_size, seed):\n",
    "    if (input_size, seed) not in _data_cache:\n",
    "        _data_cache[input_size, seed] = get_data('a', 200, input_size,\n",
    "                                                 ('042318', '043018', '051018'),\n",
    "                                                 scale=0.5,\n",
    "                                                 seed=seed)\n",
    "    return _data_cache[input_size, seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_everything():\n",
    "    dir_to_save = join(dir_dict['analyses'], 'responses_yuanyuan_8k_a_3day+maskcnn_polished_with_local_pcn+certain_configs')\n",
    "    makedirs(dir_to_save, exist_ok=True)\n",
    "    \n",
    "    old_case = 0\n",
    "    new_case = 0\n",
    "    for idx, param in enumerate(param_iterator_obj.generate()):\n",
    "        if idx % 50 == 0:\n",
    "            print(idx)\n",
    "            \n",
    "#         print(len(param))\n",
    "        \n",
    "        assert len(param) == 23\n",
    "        assert param['split_seed'] == 'legacy'\n",
    "        assert param['out_channel'] == 16\n",
    "        assert param['num_layer'] == 2\n",
    "        assert param['kernel_size_l1'] == 9\n",
    "        assert param['pooling_ksize'] == 3\n",
    "        assert param['pooling_type'] == 'avg'\n",
    "        \n",
    "#         assert param['model_seed'] == 0\n",
    "        \n",
    "        key = keygen(**{k: v for k, v in param.items() if k not in {'scale', 'smoothness'}})\n",
    "        # 10 to go.\n",
    "        result = load_training_results(key, return_model=False)\n",
    "        # load twice, first time to get the model.\n",
    "        result = load_training_results(key, return_model=True, model=build_net(result['config_extra']['model']))\n",
    "        num_epochs = [len(x) for x in result['stats_all']]\n",
    "        \n",
    "        cc_raw = np.asarray(result['stats_best']['stats']['test']['corr'])\n",
    "        assert cc_raw.shape == (79,)\n",
    "        \n",
    "        \n",
    "        datasets = get_data_cached(param['input_size'], param['split_seed'])\n",
    "\n",
    "\n",
    "        datasets = {\n",
    "            'X_train': datasets[0].astype(np.float32),\n",
    "            'y_train': datasets[1],\n",
    "            'X_val': datasets[2].astype(np.float32),\n",
    "            'y_val': datasets[3],\n",
    "            'X_test': datasets[4].astype(np.float32),\n",
    "            'y_test': datasets[5],\n",
    "        }\n",
    "\n",
    "        # only the test one is needed.\n",
    "        datasets_all = generate_datasets(\n",
    "            **datasets,\n",
    "            per_epoch_train=False,\n",
    "            shuffle_train=False,\n",
    "        )\n",
    "        shape_dict = {\n",
    "            'train': 5120,\n",
    "            'val': 1280,\n",
    "            'test': 1600,\n",
    "        }\n",
    "        \n",
    "        for subset in ('train', 'val', 'test'):\n",
    "            \n",
    "            file_to_store_this = join(dir_to_save, key, subset + '.npy')\n",
    "            if exists(file_to_store_this):\n",
    "                old_case += 1\n",
    "                continue\n",
    "                \n",
    "            new_case += 1\n",
    "                \n",
    "            makedirs(dirname(file_to_store_this), exist_ok=True)\n",
    "        \n",
    "            result_on_the_go = eval_wrapper(result['model'].cuda(),\n",
    "                                            datasets_all[subset],\n",
    "                                            'cuda',\n",
    "                                            1,\n",
    "                                            partial(eval_fn_wrapper_neural, loss_type=param['loss_type']),\n",
    "                                            (lambda dummy1,dummy2,dummy3: torch.tensor(0.0)),\n",
    "                                            return_responses=True\n",
    "                                           )\n",
    "            \n",
    "            ret = np.asarray(result_on_the_go['corr'])\n",
    "            \n",
    "            responses = np.concatenate([x[0] for x in result_on_the_go['responses']], axis=0)\n",
    "            \n",
    "            assert responses.shape == (shape_dict[subset], 79)\n",
    "            \n",
    "            # store the responses.\n",
    "            np.save(file_to_store_this, responses)\n",
    "        \n",
    "            if subset in {'test', 'val'}:\n",
    "                assert abs(result['stats_best']['stats'][subset]['corr_mean'] - result_on_the_go['corr_mean']) < 1e-5\n",
    "                assert abs(result['stats_best']['stats'][subset]['corr_mean'] - ret.mean()) < 1e-5\n",
    "                \n",
    "                # another check.\n",
    "                # make sure order of data aligns.\n",
    "                \n",
    "                label = datasets[f'y_{subset}']\n",
    "                assert responses.shape == label.shape\n",
    "                assert responses.ndim == 2\n",
    "                assert responses.shape[1] == 79\n",
    "                corr_each = np.array(\n",
    "                    [pearsonr(yhat, y)[0] if np.std(yhat) > 1e-5 and np.std(y) > 1e-5 else 0 for yhat, y in\n",
    "                          zip(responses.T, label.T)])\n",
    "                corr_each_ref = np.asarray(result_on_the_go['corr'])\n",
    "                assert corr_each.shape == corr_each_ref.shape\n",
    "                # not equal because in `eval_fn_wrapper_neural`, the label is in float32, while here it's in float64.\n",
    "                assert np.allclose(corr_each_ref, corr_each)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         _df_collect.append(\n",
    "#             {\n",
    "# #                 'split_seed': str(param['split_seed']),\n",
    "                \n",
    "                \n",
    "#                 'act_fn': param['act_fn'],\n",
    "#                 'bn_before_act': param['bn_before_act'],\n",
    "#                 'bn_after_fc': param['bn_after_fc'],\n",
    "#                 'input_size': param['input_size'],\n",
    "#                 'loss_type': param['loss_type'],\n",
    "                \n",
    "#                 # 12 to go\n",
    "                \n",
    "#                 'scale': float(param['scale']),\n",
    "#                 # \n",
    "#                 'smoothness': float(param['smoothness']),\n",
    "#                 # \n",
    "                \n",
    "#                 # 8 to go\n",
    "#                 'pcn_bn': param['pcn_bn'],\n",
    "#                 'pcn_bn_post': param['pcn_bn_post'],\n",
    "#                 'pcn_bypass': param['pcn_bypass'],\n",
    "#                 'pcn_final_act': param['pcn_final_act'],\n",
    "#                 'pcn_no_act': param['pcn_no_act'],\n",
    "#                 'pcn_bias': param['pcn_bias'],\n",
    "                \n",
    "#                 'pcn_cls': param['pcn_cls'],\n",
    "                \n",
    "#                 'model_seed': param['model_seed'],\n",
    "                \n",
    "# #                 'corr_test': result['stats_best']['stats']['test']['corr_mean'],\n",
    "#                 'corr_test': ((cc_raw/cc_max_all_neurons)**2).mean(),\n",
    "#                 'max_epoch': max(num_epochs),\n",
    "#                 'num_param': count_params(result['model']),\n",
    "#             }\n",
    "#         )\n",
    "#     _df =  pd.DataFrame(_df_collect, columns=[\n",
    "# #             'split_seed',\n",
    "#         'act_fn', 'bn_before_act', 'bn_after_fc',\n",
    "#         'input_size', 'loss_type', 'scale',\n",
    "#         'smoothness',\n",
    "\n",
    "#         'pcn_bn', 'pcn_bn_post', 'pcn_bypass', 'pcn_final_act', 'pcn_no_act', 'pcn_bias', 'pcn_cls',\n",
    "#         'model_seed',\n",
    "\n",
    "#         'corr_test', 'num_param',\n",
    "#         'max_epoch',\n",
    "\n",
    "#                                              ])\n",
    "#     _df = _df.set_index([\n",
    "#         'act_fn', 'bn_before_act', 'bn_after_fc',\n",
    "#         'input_size', 'loss_type', 'scale',\n",
    "#         'smoothness',\n",
    "#         'pcn_bn', 'pcn_bn_post', 'pcn_bypass', 'pcn_final_act', 'pcn_no_act', 'pcn_bias', 'pcn_cls',\n",
    "#         'model_seed',\n",
    "#     ],verify_integrity=True)\n",
    "    print('new', new_case)\n",
    "    print('old', old_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "new 6912\n",
      "old 0\n"
     ]
    }
   ],
   "source": [
    "df = load_everything()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
