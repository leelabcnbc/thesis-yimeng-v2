{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook does some spot checking on learned weight patterns in `scripts/training/yuanyuan_8k_a_3day/feature_approximation/local_pcn_recurrent/submit_sep2.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_v2 import dir_dict\n",
    "from thesis_v2.training.training_aux import load_training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "from os.path import join, exists, dirname\n",
    "from os import makedirs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_check = 'scripts/training/yuanyuan_8k_a_3day/feature_approximation/local_pcn_recurrent'\n",
    "path.insert(0, join(dir_dict['root'], folder_to_check))\n",
    "from submit_sep2 import param_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calc_raw_stats(weight_matrix):\n",
    "    # weight_matrix is N * D\n",
    "    # N = number of filters\n",
    "    # D = size of a (flattened) filter.\n",
    "    mean_all = weight_matrix.mean(axis=1)\n",
    "    std_all = weight_matrix.std(axis=1)\n",
    "    print('mean_all', mean_all.mean(), mean_all.std())\n",
    "    print('std_all', std_all.mean(), std_all.std())\n",
    "    \n",
    "def calc_normed_pca(weight_matrix, verbose=False):\n",
    "    # here `normed` means PCA after normalization of weight_matrix\n",
    "    \n",
    "    # there are multiple ways to define `normalization`\n",
    "    # \n",
    "    # a simple way is to make each flattened filter with unit norm.\n",
    "    weight_matrix_norm = norm(weight_matrix, axis=1, keepdims=True)\n",
    "#     assert weight_matrix_norm.shape == (256,1)\n",
    "    weight_matrix_normed = weight_matrix/weight_matrix_norm\n",
    "    \n",
    "    # tested, works as expected.\n",
    "    if verbose:\n",
    "        assert np.allclose(norm(weight_matrix_normed, axis=1), 1.0)\n",
    "    \n",
    "    \n",
    "    # then let's do PCA\n",
    "    pca_obj = PCA(svd_solver='full')\n",
    "    pca_obj.fit(weight_matrix_normed)\n",
    "    if verbose:\n",
    "        # top 10 explains 60%;\n",
    "        # top 20 explains 85%;\n",
    "        # looks fine to me, compared to https://doi.org/10.1101/677237\n",
    "        # \"Recurrent networks can recycle neural resources to flexibly trade speed for accuracy in visual recognition\"\n",
    "        \n",
    "        print(np.cumsum(pca_obj.explained_variance_ratio_)[:20])\n",
    "        print(pca_obj.components_.shape)\n",
    "    \n",
    "    # then let's return and save them.\n",
    "    return {\n",
    "        'weight_matrix_norm': weight_matrix_norm,\n",
    "        'weight_matrix': weight_matrix,\n",
    "        'components': pca_obj.components_,\n",
    "        'explained_variance_ratio': pca_obj.explained_variance_ratio_,\n",
    "    }\n",
    "\n",
    "def construct_back_by_interleave(x1, x2, by=16):\n",
    "    n1, m1 = x1.shape\n",
    "    n2, m2 = x2.shape\n",
    "    assert n1 % by == 0\n",
    "    assert n2 % by == 0\n",
    "    assert m1==m2 and n1==n2\n",
    "    \n",
    "    x1 = x1.reshape(n1//by, by, m1)\n",
    "    x2 = x2.reshape(n2//by, by, m2)\n",
    "    return np.concatenate((x1,x2), axis=1).reshape((n1+n2, m1))\n",
    "\n",
    "\n",
    "def save_one_result(filename, data):\n",
    "    makedirs(dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir\n",
    "global_vars = {\n",
    "    'save_dir': join(dir_dict['analyses'],\n",
    "                     'yuanyuan_8k_a_3day+feature_approximation+local_pcn_recurrent+submit_sep2',\n",
    "                    'pca')\n",
    "}\n",
    "\n",
    "def prepare_dir():\n",
    "    save_dir = global_vars['save_dir']\n",
    "    if not exists(save_dir):\n",
    "        makedirs(save_dir)\n",
    "prepare_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all():\n",
    "    save_dir = global_vars['save_dir']\n",
    "    count = 0\n",
    "    for idx, data in enumerate(param_iterator()):\n",
    "        count += 1\n",
    "        verbose = idx % 50 == 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        key = data['key_this_original']\n",
    "        \n",
    "        def filename_gen(x):\n",
    "            return join(save_dir, f'{idx}/unit_norm', f'{x}.pkl')\n",
    "        \n",
    "        filename_set = {'ff', 'lateral', 'all'}\n",
    "        if all([exists(filename_gen(x)) for x in filename_set]):\n",
    "            # right now if partially done, the partial results will be fully overwritten.\n",
    "            continue\n",
    "        # then load weights.\n",
    "        result = load_training_results(key, return_model=False, return_checkpoint=True)\n",
    "        # use float64 for all analysis to retain precision.\n",
    "        weight_np = result['checkpoint']['model']['moduledict.conv0.weight'].numpy().astype(np.float64)\n",
    "#         print(weight_np.dtype)\n",
    "#         print(weight_np.shape)\n",
    "        channel_shape_all = (16, 32)\n",
    "        assert len(weight_np.shape)==4 and weight_np.shape[:2] == channel_shape_all\n",
    "        \n",
    "        kernel_shape = weight_np.shape[2:]\n",
    "        channel_shape = (16, 16)\n",
    "        \n",
    "        assert kernel_shape in {(7,7), (9,9)}\n",
    "        kernel_numel = kernel_shape[0]*kernel_shape[1]\n",
    "        # according to the way I write the model (scripts/training/yuanyuan_8k_a_3day/feature_approximation/local_pcn_recurrent/master.py),\n",
    "        # in the `32` part of (16, 32, k, k), first 16 channels are input from lower layer.\n",
    "        weight_np_ff = weight_np[:,:16]\n",
    "        # rest 16 channels are input from recurrent layer.\n",
    "        weight_np_lateral = weight_np[:,16:]\n",
    "#         print(weight_np_ff.shape, weight_np_lateral.shape)\n",
    "        assert weight_np_ff.shape == weight_np_lateral.shape == channel_shape + kernel_shape\n",
    "        \n",
    "        new_shape = (channel_shape[0]*channel_shape[1], kernel_shape[0]*kernel_shape[1])\n",
    "        \n",
    "        weight_np_ff = weight_np_ff.reshape(new_shape)\n",
    "        weight_np_lateral = weight_np_lateral.reshape(new_shape)\n",
    "        \n",
    "        new_shape_all = (channel_shape_all[0]*channel_shape_all[1], kernel_shape[0]*kernel_shape[1])\n",
    "        \n",
    "        weight_np_all = weight_np.reshape(new_shape_all)\n",
    "        \n",
    "        # interleave in groups of 16.\n",
    "        \n",
    "        weight_np_all_debug = construct_back_by_interleave(weight_np_ff, weight_np_lateral)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(weight_np_all_debug.shape)\n",
    "        assert np.array_equal(\n",
    "            weight_np_all,\n",
    "            weight_np_all_debug,\n",
    "        )\n",
    "#         print(weight_np_lateral.shape, weight_np_ff.shape)\n",
    "        \n",
    "        if verbose:\n",
    "            print('idx', idx)\n",
    "            print('key', key)\n",
    "            # show some raw stats.\n",
    "            calc_raw_stats(weight_np_ff)\n",
    "            calc_raw_stats(weight_np_lateral)\n",
    "            calc_raw_stats(weight_np_all)\n",
    "        pca_res_dict = dict()\n",
    "        pca_res_dict['ff'] = calc_normed_pca(weight_np_ff, verbose)\n",
    "        pca_res_dict['lateral'] = calc_normed_pca(weight_np_lateral, verbose)\n",
    "        pca_res_dict['all'] = calc_normed_pca(weight_np_all, verbose)\n",
    "        \n",
    "        # simply store it as pickle, for simplicity.\n",
    "        \n",
    "#         data_to_save = {\n",
    "#             'idx': idx,\n",
    "#             'key': key,\n",
    "#             'ff': pca_res_ff,\n",
    "#             'lateral': pca_res_lateral,\n",
    "#             'all': pca_res_all,\n",
    "#         }\n",
    "        \n",
    "        # then save.\n",
    "        # I think it's good to save as `idx/unit_norm/ff|lateral|all/data.pkl`;\n",
    "        # this way, I can extend as needed.\n",
    "        \n",
    "        for fn in filename_set:\n",
    "            save_one_result(filename_gen(fn), pca_res_dict[fn])\n",
    "        \n",
    "    print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n",
      "key yuanyuan_8k_a_3day/feature_approximation_lpcn_recurrent_sep2/baseidx10/actrelu/lossmse/k7/bn_preTrue/model_seed0\n",
      "mean_all -0.00018880494135170006 0.006698544208436294\n",
      "std_all 0.01944413025675884 0.007153826594466825\n",
      "mean_all 0.00042352848567291416 0.007041900223180945\n",
      "std_all 0.02207085077286015 0.009327162321586481\n",
      "mean_all 0.00011736177216060698 0.006879183431866645\n",
      "std_all 0.020757490514809497 0.00841495758313068\n",
      "[0.16479427 0.258138   0.34052148 0.40926025 0.46463458 0.51096948\n",
      " 0.5451942  0.57807923 0.60954633 0.63958864 0.66515607 0.68836399\n",
      " 0.71070547 0.73058255 0.7496009  0.76711914 0.78359972 0.79837939\n",
      " 0.81274754 0.82636937]\n",
      "(49, 49)\n",
      "[0.16905511 0.26684794 0.3489334  0.40692816 0.45874946 0.50389226\n",
      " 0.54580793 0.5813355  0.61249352 0.64184447 0.66929289 0.69448178\n",
      " 0.71745058 0.73707575 0.75649949 0.77319779 0.78946152 0.80391945\n",
      " 0.81763972 0.83058278]\n",
      "(49, 49)\n",
      "[0.15808512 0.25203545 0.32993069 0.38984277 0.43881649 0.48198359\n",
      " 0.52020868 0.55481284 0.58485809 0.60940589 0.63365358 0.65646174\n",
      " 0.67891847 0.70044958 0.71990229 0.73819982 0.75547296 0.77213926\n",
      " 0.78754386 0.80172422]\n",
      "(49, 49)\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "collect_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
