{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, dirname, exists\n",
    "from os import makedirs\n",
    "from itertools import chain, islice\n",
    "\n",
    "from thesis_v2 import dir_dict\n",
    "from thesis_v2.configs.model.maskcnn_polished_with_rcnn_k_bl import (\n",
    "    explored_models_20200819_tang_generator,\n",
    "    explored_models_20200914_tang_generator,\n",
    ")\n",
    "from thesis_v2.analysis.io import collect_rcnn_k_bl_hal_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   37.6s\n",
      "/opt/conda/envs/leelab/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=-1)]: Done 600 tasks      | elapsed: 22.5min\n",
      "[Parallel(n_jobs=-1)]: Done 834 tasks      | elapsed: 31.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1104 tasks      | elapsed: 41.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1410 tasks      | elapsed: 53.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1752 tasks      | elapsed: 67.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2130 tasks      | elapsed: 81.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2544 tasks      | elapsed: 96.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2994 tasks      | elapsed: 113.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3480 tasks      | elapsed: 130.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4002 tasks      | elapsed: 149.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4560 tasks      | elapsed: 150.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5154 tasks      | elapsed: 162.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5784 tasks      | elapsed: 181.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6450 tasks      | elapsed: 201.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7152 tasks      | elapsed: 221.7min\n",
      "[Parallel(n_jobs=-1)]: Done 7890 tasks      | elapsed: 242.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8664 tasks      | elapsed: 263.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8784 out of 8784 | elapsed: 269.4min finished\n"
     ]
    }
   ],
   "source": [
    "f_main_result = join(dir_dict['analyses'], 'tang_refactored', '20200819+20200914', 'hal_tuning_complete.pkl')\n",
    "if not exists(f_main_result):\n",
    "    makedirs(dirname(f_main_result), exist_ok=True)\n",
    "    df_main_result = collect_rcnn_k_bl_hal_analysis(\n",
    "        fixed_keys = {\n",
    "        'kernel_size_l23': 3,\n",
    "        'kernel_size_l1': 9,\n",
    "        'ff_1st_block': True,\n",
    "        'pooling_ksize': 3,\n",
    "        'pooling_type': 'avg',\n",
    "        'bn_after_fc': False,\n",
    "        'rcnn_bl_psize': 1,\n",
    "        'rcnn_bl_ptype': None,\n",
    "        'input_size': 63,\n",
    "        'split_seed': 'legacy',\n",
    "        'dataset_prefix': 'tang',\n",
    "        'model_prefix': 'maskcnn_polished_with_rcnn_k_bl',\n",
    "        'scale_name': '0.01',\n",
    "        'scale': '0.01',\n",
    "        'smoothness_name': '0.000005',\n",
    "        'smoothness': '0.000005',\n",
    "    },\n",
    "        generator=chain(\n",
    "            explored_models_20200819_tang_generator(with_source=True),\n",
    "            explored_models_20200914_tang_generator(with_source=True),\n",
    "        ),\n",
    "        total_num_param=26,\n",
    "        train_size_mapping=dict(),\n",
    "    )\n",
    "    df_main_result.to_pickle(f_main_result)\n",
    "    del df_main_result\n",
    "df_main_result = pd.read_pickle(f_main_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
