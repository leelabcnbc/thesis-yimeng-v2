{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook sees how to create a feedforward approximator for recurrent features extracted in `scripts/feature_extraction/yuanyuan_8k_a/maskcnn_polished_with_local_pcn/debug.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common libs\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from sys import path\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "from thesis_v2 import dir_dict\n",
    "\n",
    "from torchnetjson.builder import build_net\n",
    "from thesis_v2.training.training_aux import load_training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_check = 'scripts/feature_extraction/yuanyuan_8k_a/maskcnn_polished_with_local_pcn'\n",
    "path.insert(0, join(dir_dict['root'], folder_to_check))\n",
    "from certain_configs import get_all_model_params, global_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_file_dir': '/my_data/thesis-yimeng-v2/results/features/maskcnn_polished_with_local_pcn/certain_configs', 'augment_config': {'module_names': ['bottomup', 'topdown', 'final'], 'name_mapping': {'moduledict.conv1.lambda_out': 'bottomup', 'moduledict.conv1.lambda_in': 'topdown', 'moduledict.final_act': 'final'}}}\n"
     ]
    }
   ],
   "source": [
    "print(global_vars)\n",
    "\n",
    "# utils\n",
    "def get_layer_idx(friendly_name):\n",
    "    return global_vars['augment_config']['module_names'].index(friendly_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all model params\n",
    "all_params_dict = get_all_model_params()\n",
    "\n",
    "# same one as in `scripts/feature_extraction/yuanyuan_8k_a/maskcnn_polished_with_local_pcn/debug.ipynb`\n",
    "model_to_check = 's_selegacy+in_sz50+out_ch16+num_l2+k_l19+k_p3+ptavg+bn_b_actTrue+bn_a_fcFalse+actrelu+p_c5+p_bypassFalse+p_n_actFalse+p_bn_pFalse+p_actTrue+p_bnTrue+p_biasTrue+sc0.01+sm0.000005+lmse+m_se0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract input 'topdown.0' as well as all 'bottomup.*' except the last one.\n",
    "\n",
    "def fetch_data(feature_file, grp_name):\n",
    "    slice_to_check = slice(None)\n",
    "    with h5py.File(feature_file, 'r') as f_feature:\n",
    "        grp = f_feature[grp_name]\n",
    "        num_bottom_up = len([x for x in grp if x.startswith(str(get_layer_idx('bottomup')) + '.')])\n",
    "        assert num_bottom_up > 2\n",
    "        \n",
    "        pcn_in = grp[str(get_layer_idx('topdown')) + '.0'][slice_to_check]\n",
    "        pcn_out_list = [grp[str(get_layer_idx('bottomup')) + f'.{x}'][slice_to_check] for x in range(num_bottom_up-1)]\n",
    "    \n",
    "    print((pcn_in.shape, pcn_in.mean(), pcn_in.std(), pcn_in.min(), pcn_in.max()))\n",
    "    print([(x.shape, x.mean(), x.std(), x.min(), x.max()) for x in pcn_out_list])\n",
    "    \n",
    "    return {\n",
    "        'in': pcn_in,\n",
    "        'out_list': pcn_out_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1600, 16, 42, 42), 0.3592737, 0.5737621, 0.0, 12.684619)\n",
      "[((1600, 16, 42, 42), 0.26013193, 0.4335558, 0.0, 9.471747), ((1600, 16, 42, 42), 0.2820749, 0.7873015, -6.068827, 17.41325), ((1600, 16, 42, 42), 0.2963904, 1.0379094, -10.003517, 23.08837), ((1600, 16, 42, 42), 0.30989194, 1.3064132, -13.968054, 28.11877), ((1600, 16, 42, 42), 0.3228603, 1.6355528, -18.504385, 30.76726), ((1600, 16, 42, 42), 0.33641696, 2.0544658, -23.724724, 32.904312)]\n"
     ]
    }
   ],
   "source": [
    "data_returned = fetch_data(join(global_vars['feature_file_dir'], model_to_check + '.hdf5'), 'X_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "def load_model(key):\n",
    "    result = load_training_results(key, return_model=False)\n",
    "    # load twice, first time to get the model.\n",
    "    model_ = load_training_results(key, return_model=True, model=build_net(result['config_extra']['model']))['model']\n",
    "    model_.cuda()\n",
    "    model_.eval()\n",
    "    return model_\n",
    "\n",
    "model = load_model(all_params_dict[model_to_check]['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "0.02194325 0.42810473 -6.068827 8.904035\n",
      "0.0\n",
      "0.0\n",
      "(0, 2)\n",
      "0.036259294 0.7234565 -10.003517 16.093918\n",
      "0.0\n",
      "0.0\n",
      "(0, 3)\n",
      "0.04976021 1.0306181 -13.968054 21.12432\n",
      "0.0\n",
      "0.0\n",
      "(0, 4)\n",
      "0.06272887 1.3919907 -18.504385 24.259983\n",
      "0.0\n",
      "0.0\n",
      "(0, 5)\n",
      "0.076285854 1.8376727 -23.724724 27.46222\n",
      "0.0\n",
      "0.0\n",
      "(1, 2)\n",
      "0.0143161295 0.32812482 -3.93469 7.189883\n",
      "0.0\n",
      "0.0\n",
      "(1, 3)\n",
      "0.027816877 0.66755944 -7.8992267 12.7758465\n",
      "0.0\n",
      "0.0\n",
      "(1, 4)\n",
      "0.04078558 1.0581534 -12.435558 17.426577\n",
      "0.0\n",
      "0.0\n",
      "(1, 5)\n",
      "0.054342493 1.5301074 -17.655897 20.628813\n",
      "0.0\n",
      "0.0\n",
      "(2, 3)\n",
      "0.013500811 0.34966654 -3.9645367 5.948244\n",
      "0.0\n",
      "0.0\n",
      "(2, 4)\n",
      "0.026469508 0.7540298 -8.500868 10.598974\n",
      "0.0\n",
      "0.0\n",
      "(2, 5)\n",
      "0.04002639 1.2418066 -13.871584 13.929815\n",
      "0.0\n",
      "0.0\n",
      "(3, 4)\n",
      "0.012968653 0.41092587 -4.618208 4.65073\n",
      "0.0\n",
      "0.0\n",
      "(3, 5)\n",
      "0.02652559 0.90977573 -10.077094 11.556656\n",
      "0.0\n",
      "0.0\n",
      "(4, 5)\n",
      "0.013556905 0.5049501 -5.458886 7.06707\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# # the idea is, given idx1 and idx2, predict out_list[idx2] - out_list[idx1]  given (out_list[idx1]  and in).\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from torch.backends import cudnn\n",
    "import torch\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "def check_similarity(d1, d2):\n",
    "    assert d1.shape == d2.shape\n",
    "    norm_diff = norm(d1-d2)/norm(d2)\n",
    "    print(norm_diff)\n",
    "    print(abs(d1-d2).max())\n",
    "    assert norm_diff < 1e-5\n",
    "\n",
    "def debug_result(model_,in_,out1,idx_diff):\n",
    "    assert idx_diff > 0\n",
    "    out_now = out1\n",
    "    with torch.no_grad():\n",
    "        for _ in range(idx_diff):\n",
    "            pred_now = model.moduledict['conv1'].forward_fb(\n",
    "                torch.tensor(out_now).cuda(),\n",
    "            ).cpu().numpy()\n",
    "            out_now = model.moduledict['conv1'].forward_update(\n",
    "                torch.tensor(out_now).cuda(),\n",
    "                torch.tensor(in_).cuda(),\n",
    "                torch.tensor(pred_now).cuda(),\n",
    "            ).cpu().numpy()\n",
    "    return out_now - out1\n",
    "\n",
    "def check_result(model_, data_dict):\n",
    "    num_out = len(data_dict['out_list'])\n",
    "    \n",
    "    for idx1 in range(num_out):\n",
    "        for idx2 in range(idx1+1, num_out):\n",
    "            print((idx1, idx2))\n",
    "            result_ref = data_dict['out_list'][idx2] - data_dict['out_list'][idx1]\n",
    "            print(result_ref.mean(), result_ref.std(), result_ref.min(), result_ref.max())\n",
    "            result_debug = debug_result(model_,data_dict['in'],data_dict['out_list'][idx1],idx2-idx1)\n",
    "            check_similarity(result_ref, result_debug)\n",
    "\n",
    "# all ok.\n",
    "check_result(model, data_returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now time to get a model to train it.\n",
    "# simple stuff. conv + relu.\n",
    "# maybe with BN.\n",
    "\n",
    "# two kinds of models\n",
    "\n",
    "# BN + conv + ReLU + BN\n",
    "# conv + ReLU + BN\n",
    "# I may want to constrain the first BN a bit,\n",
    "# say, all in_ channels share the same scale and bias; same goes with out1 channels.\n",
    "\n",
    "# some concerns: stats are different for `out_` at different iterations.\n",
    "# but let's ignore it for now.\n",
    "from thesis_v2.models.feature_approximation.builder import (\n",
    "    gen_local_pcn_recurrent_feature_approximator\n",
    ")\n",
    "\n",
    "from thesis_v2.training_extra.feature_approximation.opt import get_feature_approximation_opt_config\n",
    "from thesis_v2.training_extra.feature_approximation.training import train_one\n",
    "\n",
    "def handle_one_case(*,\n",
    "                    data_dict,\n",
    "                    kernel_size,\n",
    "                    note,\n",
    "                    batchnorm_pre=True,\n",
    "                    batchnorm_post=True,\n",
    "                    act_fn='relu',\n",
    "                   ):\n",
    "    \n",
    "    # prepare dataset\n",
    "    num_out = len(data_dict['out_list'])\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for idx1 in range(num_out):\n",
    "        for idx2 in range(idx1+1, num_out):\n",
    "            if idx2 - idx1 != 2:\n",
    "                continue\n",
    "            print((idx1, idx2))\n",
    "            x_train.append(np.concatenate([data_dict['in'],data_dict['out_list'][idx1]], axis=1))\n",
    "            y_train.append(data_dict['out_list'][idx2] - data_dict['out_list'][idx1])\n",
    "    \n",
    "    x_train = np.concatenate(x_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    \n",
    "    print((x_train.shape, y_train.shape))\n",
    "    \n",
    "    dataset_this = {\n",
    "        'X_train': x_train,\n",
    "        'y_train': y_train,\n",
    "    }\n",
    "    \n",
    "    def gen_cnn_partial(in_shape):\n",
    "        # I assume two inputs have the same number of channels and shapes.\n",
    "        assert len(in_shape) == 3\n",
    "        assert in_shape[0] % 2 == 0\n",
    "        return gen_local_pcn_recurrent_feature_approximator(\n",
    "            in_shape_lower=[in_shape[0]//2, in_shape[1], in_shape[2]],\n",
    "            in_shape_higher=[in_shape[0]//2, in_shape[1], in_shape[2]],\n",
    "            kernel_size=kernel_size,\n",
    "            act_fn=act_fn,\n",
    "        )\n",
    "    #\n",
    "    res = train_one(arch_json_partial=gen_cnn_partial,\n",
    "                    opt_config_partial=get_feature_approximation_opt_config,\n",
    "                    datasets=dataset_this,\n",
    "                    # note this gets saved under v1 folder...\n",
    "                    # but it should not matter.\n",
    "                    key=f'debug/feature_approximation/local_pcn_recurrent_feature_approximator/note{note}/kernel_size{kernel_size}/act_fn{act_fn}/batchnorm_pre{batchnorm_pre}/batchnorm_post{batchnorm_post}',\n",
    "                    show_every=1000,\n",
    "                    model_seed=0, return_model=False)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2)\n",
      "(1, 3)\n",
      "(2, 4)\n",
      "(3, 5)\n",
      "((6400, 32, 42, 42), (6400, 16, 42, 42))\n",
      "num_param 41584\n",
      "========starting phase 1/3==========\n",
      "========starting epoch 0==========\n",
      "0-0, train loss 1.6306195259094238\n",
      "train loss 1.6306195259094238\n",
      "========done epoch 0==========\n",
      "========starting epoch 1000==========\n",
      "1000-0, train loss 0.07750361412763596\n",
      "train loss 0.07750361412763596\n",
      "========done epoch 1000==========\n",
      "========starting epoch 2000==========\n",
      "2000-0, train loss 0.05282807722687721\n",
      "train loss 0.05282807722687721\n",
      "========done epoch 2000==========\n",
      "========starting epoch 3000==========\n",
      "3000-0, train loss 0.04851727560162544\n",
      "train loss 0.04851727560162544\n",
      "========done epoch 3000==========\n",
      "========starting epoch 4000==========\n",
      "4000-0, train loss 0.050110407173633575\n",
      "train loss 0.050110407173633575\n",
      "========done epoch 4000==========\n",
      "========starting epoch 5000==========\n",
      "5000-0, train loss 0.049012117087841034\n",
      "train loss 0.049012117087841034\n",
      "========done epoch 5000==========\n",
      "========starting epoch 6000==========\n",
      "6000-0, train loss 0.047640301287174225\n",
      "train loss 0.047640301287174225\n",
      "========done epoch 6000==========\n",
      "========starting epoch 7000==========\n",
      "7000-0, train loss 0.0482809953391552\n",
      "train loss 0.0482809953391552\n",
      "========done epoch 7000==========\n",
      "========starting epoch 8000==========\n",
      "8000-0, train loss 0.0482926145195961\n",
      "train loss 0.0482926145195961\n",
      "========done epoch 8000==========\n",
      "========starting epoch 9000==========\n",
      "9000-0, train loss 0.0502128005027771\n",
      "train loss 0.0502128005027771\n",
      "========done epoch 9000==========\n",
      "========starting epoch 10000==========\n",
      "10000-0, train loss 0.0504579097032547\n",
      "train loss 0.0504579097032547\n",
      "========done epoch 10000==========\n",
      "========starting epoch 11000==========\n",
      "11000-0, train loss 0.05106149613857269\n",
      "train loss 0.05106149613857269\n",
      "========done epoch 11000==========\n",
      "========starting epoch 12000==========\n",
      "12000-0, train loss 0.0500941164791584\n",
      "train loss 0.0500941164791584\n",
      "========done epoch 12000==========\n",
      "========starting epoch 13000==========\n",
      "13000-0, train loss 0.047540806233882904\n",
      "train loss 0.047540806233882904\n",
      "========done epoch 13000==========\n",
      "========starting epoch 14000==========\n",
      "14000-0, train loss 0.04901675507426262\n",
      "train loss 0.04901675507426262\n",
      "========done epoch 14000==========\n",
      "========starting epoch 15000==========\n",
      "15000-0, train loss 0.05001739040017128\n",
      "train loss 0.05001739040017128\n",
      "========done epoch 15000==========\n",
      "========starting epoch 16000==========\n",
      "16000-0, train loss 0.048771101981401443\n",
      "train loss 0.048771101981401443\n",
      "========done epoch 16000==========\n",
      "========starting epoch 17000==========\n",
      "17000-0, train loss 0.05214261636137962\n",
      "train loss 0.05214261636137962\n",
      "========done epoch 17000==========\n",
      "========starting epoch 18000==========\n",
      "18000-0, train loss 0.049269575625658035\n",
      "train loss 0.049269575625658035\n",
      "========done epoch 18000==========\n",
      "========starting epoch 19000==========\n",
      "19000-0, train loss 0.04995778203010559\n",
      "train loss 0.04995778203010559\n",
      "========done epoch 19000==========\n",
      "========end phase 1/3==========\n",
      "========starting phase 2/3==========\n",
      "for grp of sz 6, lr from 0.001000 to 0.000333\n",
      "========starting epoch 0==========\n",
      "0-0, train loss 0.047398097813129425\n",
      "train loss 0.047398097813129425\n",
      "========done epoch 0==========\n",
      "========starting epoch 1000==========\n",
      "1000-0, train loss 0.04761982709169388\n",
      "train loss 0.04761982709169388\n",
      "========done epoch 1000==========\n",
      "========starting epoch 2000==========\n",
      "2000-0, train loss 0.047740522772073746\n",
      "train loss 0.047740522772073746\n",
      "========done epoch 2000==========\n",
      "========starting epoch 3000==========\n",
      "3000-0, train loss 0.04798625409603119\n",
      "train loss 0.04798625409603119\n",
      "========done epoch 3000==========\n",
      "========starting epoch 4000==========\n",
      "4000-0, train loss 0.04962059110403061\n",
      "train loss 0.04962059110403061\n",
      "========done epoch 4000==========\n",
      "========starting epoch 5000==========\n",
      "5000-0, train loss 0.049356330186128616\n",
      "train loss 0.049356330186128616\n",
      "========done epoch 5000==========\n",
      "========starting epoch 6000==========\n",
      "6000-0, train loss 0.04918663203716278\n",
      "train loss 0.04918663203716278\n",
      "========done epoch 6000==========\n",
      "========starting epoch 7000==========\n",
      "7000-0, train loss 0.049945782870054245\n",
      "train loss 0.049945782870054245\n",
      "========done epoch 7000==========\n",
      "========starting epoch 8000==========\n",
      "8000-0, train loss 0.049353308975696564\n",
      "train loss 0.049353308975696564\n",
      "========done epoch 8000==========\n",
      "========starting epoch 9000==========\n",
      "9000-0, train loss 0.048312775790691376\n",
      "train loss 0.048312775790691376\n",
      "========done epoch 9000==========\n",
      "========starting epoch 10000==========\n",
      "10000-0, train loss 0.04784580320119858\n",
      "train loss 0.04784580320119858\n",
      "========done epoch 10000==========\n",
      "========starting epoch 11000==========\n",
      "11000-0, train loss 0.048700980842113495\n",
      "train loss 0.048700980842113495\n",
      "========done epoch 11000==========\n",
      "========starting epoch 12000==========\n",
      "12000-0, train loss 0.0470561645925045\n",
      "train loss 0.0470561645925045\n",
      "========done epoch 12000==========\n",
      "========starting epoch 13000==========\n",
      "13000-0, train loss 0.04943963885307312\n",
      "train loss 0.04943963885307312\n",
      "========done epoch 13000==========\n",
      "========starting epoch 14000==========\n",
      "14000-0, train loss 0.048814885318279266\n",
      "train loss 0.048814885318279266\n",
      "========done epoch 14000==========\n",
      "========starting epoch 15000==========\n",
      "15000-0, train loss 0.04848523437976837\n",
      "train loss 0.04848523437976837\n",
      "========done epoch 15000==========\n",
      "========starting epoch 16000==========\n",
      "16000-0, train loss 0.04829274117946625\n",
      "train loss 0.04829274117946625\n",
      "========done epoch 16000==========\n",
      "========starting epoch 17000==========\n",
      "17000-0, train loss 0.04863720387220383\n",
      "train loss 0.04863720387220383\n",
      "========done epoch 17000==========\n",
      "========starting epoch 18000==========\n",
      "18000-0, train loss 0.04846165329217911\n",
      "train loss 0.04846165329217911\n",
      "========done epoch 18000==========\n",
      "========starting epoch 19000==========\n",
      "19000-0, train loss 0.04731791466474533\n",
      "train loss 0.04731791466474533\n",
      "========done epoch 19000==========\n",
      "========end phase 2/3==========\n",
      "========starting phase 3/3==========\n",
      "for grp of sz 6, lr from 0.000333 to 0.000111\n",
      "========starting epoch 0==========\n",
      "0-0, train loss 0.048782989382743835\n",
      "train loss 0.048782989382743835\n",
      "========done epoch 0==========\n",
      "========starting epoch 1000==========\n",
      "1000-0, train loss 0.04963615909218788\n",
      "train loss 0.04963615909218788\n",
      "========done epoch 1000==========\n",
      "========starting epoch 2000==========\n",
      "2000-0, train loss 0.049389809370040894\n",
      "train loss 0.049389809370040894\n",
      "========done epoch 2000==========\n",
      "========starting epoch 3000==========\n",
      "3000-0, train loss 0.048107217997312546\n",
      "train loss 0.048107217997312546\n",
      "========done epoch 3000==========\n",
      "========starting epoch 4000==========\n",
      "4000-0, train loss 0.048950355499982834\n",
      "train loss 0.048950355499982834\n",
      "========done epoch 4000==========\n",
      "========starting epoch 5000==========\n",
      "5000-0, train loss 0.04812944680452347\n",
      "train loss 0.04812944680452347\n",
      "========done epoch 5000==========\n",
      "========starting epoch 6000==========\n",
      "6000-0, train loss 0.04996785894036293\n",
      "train loss 0.04996785894036293\n",
      "========done epoch 6000==========\n",
      "========starting epoch 7000==========\n",
      "7000-0, train loss 0.04808470979332924\n",
      "train loss 0.04808470979332924\n",
      "========done epoch 7000==========\n",
      "========starting epoch 8000==========\n",
      "8000-0, train loss 0.04919516667723656\n",
      "train loss 0.04919516667723656\n",
      "========done epoch 8000==========\n",
      "========starting epoch 9000==========\n",
      "9000-0, train loss 0.04862317070364952\n",
      "train loss 0.04862317070364952\n",
      "========done epoch 9000==========\n",
      "========starting epoch 10000==========\n",
      "10000-0, train loss 0.047625862061977386\n",
      "train loss 0.047625862061977386\n",
      "========done epoch 10000==========\n",
      "========starting epoch 11000==========\n",
      "11000-0, train loss 0.04902207851409912\n",
      "train loss 0.04902207851409912\n",
      "========done epoch 11000==========\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-461994338785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_returned\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'debug'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-9-2e8fc793b463>\u001b[0m in \u001b[0;36mhandle_one_case\u001b[0;34m(data_dict, kernel_size, note, batchnorm_pre, batchnorm_post, act_fn)\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'debug/feature_approximation/local_pcn_recurrent_feature_approximator/note{note}/kernel_size{kernel_size}/act_fn{act_fn}/batchnorm_pre{batchnorm_pre}/batchnorm_post{batchnorm_post}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mshow_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     model_seed=0, return_model=False)\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training_extra/feature_approximation/training.py\u001b[0m in \u001b[0;36mtrain_one\u001b[0;34m(arch_json_partial, opt_config_partial, datasets, key, show_every, model_seed, train_seed, max_epoch, device, return_model)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mreturn_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         extra_params={\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;34m'datasets'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'y_dim'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         },\n\u001b[1;32m     52\u001b[0m     )\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training_extra/training.py\u001b[0m in \u001b[0;36mtrain_one_wrapper\u001b[0;34m(get_json_fn, initialize_model_fn, get_optimizer_fn, get_loss_fn, datasets, key, show_every, model_seed, train_seed, max_epoch, early_stopping_field, device, val_test_every, return_model, extra_params)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mextra_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     )\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training_extra/training.py\u001b[0m in \u001b[0;36mtrain_one_inner\u001b[0;34m(model, datasets, key, optimizer, loss_fn, config_extra, seed, config, eval_fn, return_model, shuffle_train, extra_params)\u001b[0m\n\u001b[1;32m     55\u001b[0m                             \u001b[0meval_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                             legacy_random_seed=True)\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training_aux.py\u001b[0m in \u001b[0;36mtraining_wrapper\u001b[0;34m(model, loss_fn, optimizer, dataset_train, dataset_val, dataset_test, config, config_extra, eval_fn, key, return_model, deterministic, legacy_random_seed)\u001b[0m\n\u001b[1;32m    187\u001b[0m                            \u001b[0meval_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_normed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                            \u001b[0mf_best\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_best_tmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                            legacy_random_seed=legacy_random_seed)\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0;31m# then copy/move that file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_best_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_func, optimizer, config, f_best, dataset_train, dataset_val, dataset_test, eval_fn, legacy_random_seed)\u001b[0m\n\u001b[1;32m    123\u001b[0m                                                        \u001b[0mstats_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                                                        \u001b[0mf_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_phase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                                                        legacy_random_seed)\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mstats_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats_this_phase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'========end phase {i_phase + 1}/{num_phase}=========='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training.py\u001b[0m in \u001b[0;36mtrain_one_phase\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                             \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                                             \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                                             dataset_train, print_flag)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         stats_this_batch['val'] = _val_test_one_epoch(i_epoch, model,\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training.py\u001b[0m in \u001b[0;36m_train_one_epoch\u001b[0;34m(i_epoch, model, optimizer, loss_func, conf, dataset_train, print_flag)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# not unpacked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training_extra/feature_approximation/loss.py\u001b[0m in \u001b[0;36mloss_func_inner\u001b[0;34m(yhat, y, model)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopt_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mopt_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2256\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2257\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "handle_one_case(\n",
    "    data_dict=data_returned,\n",
    "    kernel_size=9,\n",
    "    note='debug',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
