{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this file is to make sure that feature extraction of `maskcnn_polished_with_local_pcn` added in <https://github.com/leelabcnbc/thesis-yimeng-v2/commit/92c22a57c8bc72286eec4d9d03e2acb79ab5ab26> works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, check that adding these lambda stuffs do not change response at all.\n",
    "\n",
    "\n",
    "from torchnetjson.builder import build_net\n",
    "from thesis_v2.training.training_aux import load_training_results\n",
    "from thesis_v2 import dir_dict\n",
    "from thesis_v2.models.maskcnn_polished_with_local_pcn.builder import load_modules\n",
    "\n",
    "load_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "from os.path import join, exists\n",
    "folder_to_check = 'scripts/training/yuanyuan_8k_a_3day/maskcnn_polished_with_local_pcn'\n",
    "path.insert(0, join(dir_dict['root'], folder_to_check))\n",
    "from submit_certain_configs import param_iterator_obj\n",
    "from key_utils import keygen, script_keygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('split_seed', 'legacy'), ('model_seed', 0), ('act_fn', 'relu'), ('loss_type', 'mse'), ('input_size', 50), ('out_channel', 16), ('num_layer', 2), ('kernel_size_l1', 9), ('pooling_ksize', 3), ('pooling_type', 'avg'), ('bn_before_act', True), ('bn_after_fc', False), ('scale_name', '0.01'), ('scale', '0.01'), ('smoothness_name', '0.000005'), ('smoothness', '0.000005'), ('pcn_bn', True), ('pcn_bn_post', False), ('pcn_bypass', False), ('pcn_cls', 5), ('pcn_final_act', True), ('pcn_no_act', False), ('pcn_bias', True)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_one_model():\n",
    "    for idx, param in enumerate(param_iterator_obj.generate()):\n",
    "        # let's use a fully recurrent one for debugging.\n",
    "        if param['pcn_cls'] != 5:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        assert len(param) == 23\n",
    "        assert param['split_seed'] == 'legacy'\n",
    "        assert param['out_channel'] == 16\n",
    "        assert param['num_layer'] == 2\n",
    "        assert param['kernel_size_l1'] == 9\n",
    "        assert param['pooling_ksize'] == 3\n",
    "        assert param['pooling_type'] == 'avg'\n",
    "        print(param)\n",
    "\n",
    "    #         assert param['model_seed'] == 0\n",
    "\n",
    "        key = keygen(**{k: v for k, v in param.items() if k not in {'scale', 'smoothness'}})\n",
    "        key_script = script_keygen(**param)\n",
    "        # 10 to go.\n",
    "        result_ = load_training_results(key, return_model=False)\n",
    "        # load twice, first time to get the model.\n",
    "        result_ = load_training_results(key, return_model=True, model=build_net(result_['config_extra']['model']))\n",
    "        num_epochs = [len(x) for x in result_['stats_all']]\n",
    "\n",
    "        cc_raw = np.asarray(result_['stats_best']['stats']['test']['corr'])\n",
    "        \n",
    "        return {\n",
    "            'key': key,\n",
    "            'key_script': key_script,\n",
    "            'param': param,\n",
    "            'result': result_,\n",
    "        }\n",
    "    \n",
    "result = get_one_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s_selegacy+in_sz50+out_ch16+num_l2+k_l19+k_p3+ptavg+bn_b_actTrue+bn_a_fcFalse+actrelu+p_c5+p_bypassFalse+p_n_actFalse+p_bn_pFalse+p_actTrue+p_bnTrue+p_biasTrue+sc0.01+sm0.000005+lmse+m_se0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['key_script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yuanyuan_8k_a_3day/maskcnn_polished_with_local_pcn/s_selegacy/in_sz50/out_ch16/num_l2/k_l19/k_p3/ptavg/bn_b_actTrue/bn_a_fcFalse/actrelu/p_c5/p_bypassFalse/p_n_actFalse/p_bn_pFalse/p_actTrue/p_bnTrue/p_biasTrue/sc0.01/sm0.000005/lmse/m_se0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JSONNet(\n",
       "  (moduledict): ModuleDict(\n",
       "    (act0): ReLU()\n",
       "    (act1): ReLU()\n",
       "    (bn0): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn_input): BatchNorm2d(1, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv0): Conv2d(1, 16, kernel_size=(9, 9), stride=(1, 1), bias=False)\n",
       "    (conv1): PcConvBp(\n",
       "      (lambda_in): LambdaSingle()\n",
       "      (lambda_out): LambdaSingle()\n",
       "      (FFconv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (FBconv): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (act_fn): ReLU(inplace)\n",
       "    )\n",
       "    (fc): FactoredLinear2D()\n",
       "    (final_act): ReLU()\n",
       "    (pooling): AvgPool2d(kernel_size=3, stride=3, padding=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_v2.data.prepared.yuanyuan_8k import get_data\n",
    "from thesis_v2.training_extra.data import generate_datasets\n",
    "from thesis_v2.training.training import eval_wrapper\n",
    "\n",
    "from functools import partial\n",
    "from thesis_v2.training_extra.evaluation import eval_fn_wrapper as eval_fn_wrapper_neural\n",
    "import torch\n",
    "\n",
    "\n",
    "def eval_again(param, model, stats_best):\n",
    "    datasets = get_data('a', 200, param['input_size'], ('042318', '043018', '051018'), scale=0.5,\n",
    "                        seed=param['split_seed'])\n",
    "    datasets = {\n",
    "        'X_train': datasets[0].astype(np.float32),\n",
    "        'y_train': datasets[1],\n",
    "        'X_val': datasets[2].astype(np.float32),\n",
    "        'y_val': datasets[3],\n",
    "        'X_test': datasets[4].astype(np.float32),\n",
    "        'y_test': datasets[5],\n",
    "    }\n",
    "    \n",
    "    datasets_to_return = {\n",
    "        'X_test': datasets['X_test'],\n",
    "        'y_test': datasets['y_test'],\n",
    "    }\n",
    "    \n",
    "    # only the test one is needed.\n",
    "    datasets = generate_datasets(\n",
    "        **datasets,\n",
    "        per_epoch_train=True, shuffle_train=True,\n",
    "    )['test']\n",
    "    \n",
    "    result_on_the_go = eval_wrapper(model.cuda(),\n",
    "                                    datasets,\n",
    "                                    'cuda',\n",
    "                                    1,\n",
    "                                    partial(eval_fn_wrapper_neural, loss_type=param['loss_type']),\n",
    "                                        lambda dummy1,dummy2,dummy3: torch.tensor(0.0)\n",
    "                                   )\n",
    "    corrs = np.asarray(result_on_the_go['corr'])\n",
    "    corr_ref = np.asarray(stats_best['stats']['test']['corr'])\n",
    "    assert corrs.shape == corr_ref.shape == (79,)\n",
    "    \n",
    "    assert abs(corrs-corr_ref).max() < 1e-6\n",
    "    \n",
    "    return datasets_to_return\n",
    "    \n",
    "\n",
    "datasets_for_debug = eval_again(result['param'], result['result']['model'], result['result']['stats_best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num batch 7\n",
      "working on 0 to 256 of 1600\n",
      "working on 256 to 512 of 1600\n",
      "working on 512 to 768 of 1600\n",
      "working on 768 to 1024 of 1600\n",
      "working on 1024 to 1280 of 1600\n",
      "working on 1280 to 1536 of 1600\n",
      "working on 1536 to 1600 of 1600\n"
     ]
    }
   ],
   "source": [
    "global_vars = {\n",
    "    'feature_file': join(dir_dict['features'],\n",
    "                            'maskcnn_polished_with_local_pcn',\n",
    "                            'debug.hdf5'\n",
    "                            ),\n",
    "    'augment_config': {\n",
    "        'module_names': ['bottomup', 'topdown', 'final'],\n",
    "        'name_mapping': {\n",
    "            'moduledict.conv1.lambda_out': 'bottomup',\n",
    "            'moduledict.conv1.lambda_in': 'topdown',\n",
    "            'moduledict.final_act': 'final',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "from os import makedirs\n",
    "from os.path import dirname\n",
    "from thesis_v2.feature_extraction.extraction import extract_features\n",
    "import h5py\n",
    "from torch import tensor\n",
    "\n",
    "def augment_modules(model, datasets_to_extract, file_to_save, grp_name='features'):\n",
    "    augment_config = global_vars['augment_config']\n",
    "    makedirs(dirname(file_to_save), exist_ok=True)\n",
    "    with h5py.File(file_to_save) as f_feature:\n",
    "        if grp_name not in f_feature:\n",
    "            grp = f_feature.create_group(grp_name)\n",
    "\n",
    "            extract_features(model, (datasets_to_extract,),\n",
    "                             preprocessor=lambda x: (tensor(x[0]).cuda(),),\n",
    "                             output_group=grp,\n",
    "                             batch_size=256,\n",
    "                             augment_config=augment_config,\n",
    "                             # mostly for replicating old results\n",
    "                             deterministic=True\n",
    "                             )\n",
    "            \n",
    "augment_modules(result['result']['model'].eval(), datasets_for_debug['X_test'],\n",
    "                global_vars['feature_file'], 'X_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to verify that these saved feature make sense.\n",
    "\n",
    "\n",
    "def get_layer_idx(friendly_name):\n",
    "    return global_vars['augment_config']['module_names'].index(friendly_name)\n",
    "\n",
    "def expected_block_list():\n",
    "    # bottom up should have 5 + 2 (1 for initial ff, 1 for last )\n",
    "    # topdown should have 5 + 1 (1 for initial input)\n",
    "    return {f\"\"\"{get_layer_idx('bottomup')}.{x}\"\"\" for x in range(7)} | \\\n",
    "           {f\"\"\"{get_layer_idx('topdown')}.{x}\"\"\" for x in range(6)} | \\\n",
    "           {f\"\"\"{get_layer_idx('final')}.{x}\"\"\" for x in range(1)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '1.0', '1.1', '1.2', '1.3', '1.4', '1.5', '2.0']\n",
      "(10, 16, 42, 42)\n",
      "(10, 16, 42, 42)\n",
      "(10, 16, 42, 42)\n",
      "[(10, 16, 42, 42), (10, 16, 42, 42), (10, 16, 42, 42), (10, 16, 42, 42), (10, 16, 42, 42)]\n",
      "[(10, 16, 42, 42), (10, 16, 42, 42), (10, 16, 42, 42), (10, 16, 42, 42), (10, 16, 42, 42)]\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0\n",
      "0.0\n",
      "0.0\n",
      "1\n",
      "0.0\n",
      "0.0\n",
      "2\n",
      "0.0\n",
      "0.0\n",
      "3\n",
      "0.0\n",
      "0.0\n",
      "4\n",
      "0.0\n",
      "0.0\n",
      "0\n",
      "0.0\n",
      "0.0\n",
      "1\n",
      "0.0\n",
      "0.0\n",
      "2\n",
      "0.0\n",
      "0.0\n",
      "3\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# let's check output `final` first. this is easy to check.\n",
    "from numpy.linalg import norm\n",
    "from torch.backends import cudnn\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "def check_similarity(d1, d2):\n",
    "    assert d1.shape == d2.shape\n",
    "    norm_diff = norm(d1-d2)/norm(d2)\n",
    "    print(norm_diff)\n",
    "    print(abs(d1-d2).max())\n",
    "    assert norm_diff < 1e-5\n",
    "\n",
    "def check_outputs(feature_file, corr_ref, y_data, loss_type, model, grp_name='features',):\n",
    "    with h5py.File(feature_file, 'r') as f_feature:\n",
    "        # check keys\n",
    "        block_list = list(f_feature[grp_name].keys())\n",
    "        \n",
    "        print(block_list)\n",
    "        assert set(block_list) == expected_block_list()\n",
    "                                  \n",
    "    with h5py.File(feature_file, 'r') as f_feature:\n",
    "        y_hat = f_feature[grp_name][str(get_layer_idx('final')) + '.0'][()]\n",
    "    assert y_hat.shape == y_data.shape == (1600, 79)\n",
    "    ret_dict = eval_fn_wrapper_neural(yhat_all=[[y_hat]], y_all=[[y_data]],loss_type=loss_type)\n",
    "    \n",
    "    corr = np.asarray(ret_dict['corr'])\n",
    "    assert corr_ref.shape == corr.shape == (79,)\n",
    "    \n",
    "    assert abs(corr-corr_ref).max() < 1e-6\n",
    "    \n",
    "    # let's collect first 10 images' responses for verification.\n",
    "    slice_to_check = slice(0, 10)\n",
    "    with h5py.File(feature_file, 'r') as f_feature:\n",
    "        pcn_in = f_feature[grp_name][str(get_layer_idx('topdown')) + '.0'][slice_to_check]\n",
    "        pcn_ffinit = f_feature[grp_name][str(get_layer_idx('bottomup')) + '.0'][slice_to_check]\n",
    "        pcn_final = f_feature[grp_name][str(get_layer_idx('bottomup')) + '.6'][slice_to_check]\n",
    "        \n",
    "        pcn_pred_list = [f_feature[grp_name][str(get_layer_idx('topdown')) + f'.{x}'][slice_to_check] for x in range(1,6)]\n",
    "        pcn_out_list = [f_feature[grp_name][str(get_layer_idx('bottomup')) + f'.{x}'][slice_to_check] for x in range(1,6)]\n",
    "    \n",
    "    print(pcn_in.shape)\n",
    "    print(pcn_ffinit.shape)\n",
    "    print(pcn_final.shape)\n",
    "    print([x.shape for x in pcn_pred_list]) \n",
    "    print([x.shape for x in pcn_out_list])\n",
    "    \n",
    "    # then let's check 'topdown' and 'bottomup'.\n",
    "    # first, let's see if we can use pcn_in to get pcn_ffinit\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pcn_ffinit_debug = model.moduledict['conv1'].forward_init(torch.tensor(pcn_in).cuda()).cpu().numpy()\n",
    "    check_similarity(pcn_ffinit, pcn_ffinit_debug)\n",
    "    \n",
    "    # then check pcn_final to actual final.\n",
    "    with torch.no_grad():\n",
    "        final_debug = model.moduledict['final_act'](\n",
    "            model.moduledict['fc'](\n",
    "                model.moduledict['pooling'](\n",
    "                    model.moduledict['act1'](\n",
    "                        model.moduledict['bn1'](\n",
    "                            torch.tensor(pcn_final).cuda()\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ).cpu().numpy()\n",
    "    check_similarity(y_hat[slice_to_check], final_debug)\n",
    "    \n",
    "    \n",
    "    # then check from pcn_ffinit to pcn_pred_list[0]. in this particular case,\n",
    "    # this is not very interesting, as forward_post is empty (I set pcn_bypass=True in all my tried params,\n",
    "    # probably to simplify analysis, I assume?)\n",
    "    with torch.no_grad():\n",
    "        pcn_pred_0_debug = model.moduledict['conv1'].forward_fb(torch.tensor(pcn_ffinit).cuda()).cpu().numpy()\n",
    "    check_similarity(pcn_pred_list[0], pcn_pred_0_debug)\n",
    "    # then check from pcn_out_list[-1] to pcn_final\n",
    "    with torch.no_grad():\n",
    "        pcn_final_debug = model.moduledict['conv1'].forward_post(\n",
    "            torch.tensor(pcn_out_list[-1]).cuda(),\n",
    "            torch.tensor(pcn_in).cuda()\n",
    "        ).cpu().numpy()\n",
    "    check_similarity(pcn_final, pcn_final_debug)\n",
    "    \n",
    "    # from pcn_pred_list[x] to pcn_out_list[x]\n",
    "    assert len(pcn_pred_list) == len(pcn_out_list) == 5\n",
    "    out_prev = pcn_ffinit\n",
    "    for idx, (pred_this, out_this) in enumerate(zip(pcn_pred_list, pcn_out_list)):\n",
    "        print(idx)\n",
    "        with torch.no_grad():\n",
    "            out_this_debug = model.moduledict['conv1'].forward_update(\n",
    "                torch.tensor(out_prev).cuda(),\n",
    "                torch.tensor(pcn_in).cuda(),\n",
    "                torch.tensor(pred_this).cuda(),\n",
    "            ).cpu().numpy()\n",
    "        check_similarity(out_this, out_this_debug)\n",
    "        out_prev = out_this\n",
    "    \n",
    "    # from pcn_out_list[x] to pcn_pred_list[x+1]\n",
    "    for idx, (pred_this, out_this) in enumerate(zip(pcn_pred_list[1:], pcn_out_list[:-1])):\n",
    "        print(idx)\n",
    "        with torch.no_grad():\n",
    "            pred_this_debug = model.moduledict['conv1'].forward_fb(\n",
    "                torch.tensor(out_this).cuda(),\n",
    "            ).cpu().numpy()\n",
    "        check_similarity(pred_this, pred_this_debug)\n",
    "    \n",
    "check_outputs(global_vars['feature_file'], np.asarray(result['result']['stats_best']['stats']['test']['corr']),\n",
    "              datasets_for_debug['y_test'], result['param']['loss_type'], result['result']['model'], 'X_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
