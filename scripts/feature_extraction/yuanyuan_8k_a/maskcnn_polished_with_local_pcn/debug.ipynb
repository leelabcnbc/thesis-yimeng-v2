{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this file is to make sure that feature extraction of `maskcnn_polished_with_local_pcn` added in <https://github.com/leelabcnbc/thesis-yimeng-v2/commit/92c22a57c8bc72286eec4d9d03e2acb79ab5ab26> works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, check that adding these lambda stuffs do not change response at all.\n",
    "\n",
    "\n",
    "from torchnetjson.builder import build_net\n",
    "from thesis_v2.training.training_aux import load_training_results\n",
    "from thesis_v2 import dir_dict\n",
    "from thesis_v2.models.maskcnn_polished_with_local_pcn.builder import load_modules\n",
    "\n",
    "load_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "from os.path import join\n",
    "folder_to_check = 'scripts/training/yuanyuan_8k_a_3day/maskcnn_polished_with_local_pcn'\n",
    "path.insert(0, join(dir_dict['root'], folder_to_check))\n",
    "from submit_certain_configs import param_iterator_obj\n",
    "from key_utils import keygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('split_seed', 'legacy'), ('model_seed', 0), ('act_fn', 'relu'), ('loss_type', 'mse'), ('input_size', 50), ('out_channel', 16), ('num_layer', 2), ('kernel_size_l1', 9), ('pooling_ksize', 3), ('pooling_type', 'avg'), ('bn_before_act', True), ('bn_after_fc', False), ('scale_name', '0.01'), ('scale', '0.01'), ('smoothness_name', '0.000005'), ('smoothness', '0.000005'), ('pcn_bn', True), ('pcn_bn_post', False), ('pcn_bypass', False), ('pcn_cls', 0), ('pcn_final_act', True), ('pcn_no_act', False), ('pcn_bias', True)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_one_model():\n",
    "    for idx, param in enumerate(param_iterator_obj.generate()):\n",
    "        assert len(param) == 23\n",
    "        assert param['split_seed'] == 'legacy'\n",
    "        assert param['out_channel'] == 16\n",
    "        assert param['num_layer'] == 2\n",
    "        assert param['kernel_size_l1'] == 9\n",
    "        assert param['pooling_ksize'] == 3\n",
    "        assert param['pooling_type'] == 'avg'\n",
    "\n",
    "    #         assert param['model_seed'] == 0\n",
    "\n",
    "        key = keygen(**{k: v for k, v in param.items() if k not in {'scale', 'smoothness'}})\n",
    "        # 10 to go.\n",
    "        result_ = load_training_results(key, return_model=False)\n",
    "        # load twice, first time to get the model.\n",
    "        result_ = load_training_results(key, return_model=True, model=build_net(result_['config_extra']['model']))\n",
    "        num_epochs = [len(x) for x in result_['stats_all']]\n",
    "\n",
    "        cc_raw = np.asarray(result_['stats_best']['stats']['test']['corr'])\n",
    "        \n",
    "        print(param)\n",
    "        \n",
    "        return {\n",
    "            'key': key,\n",
    "            'param': param,\n",
    "            'result': result_,\n",
    "        }\n",
    "result = get_one_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JSONNet(\n",
       "  (moduledict): ModuleDict(\n",
       "    (act0): ReLU()\n",
       "    (act1): ReLU()\n",
       "    (bn0): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn_input): BatchNorm2d(1, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv0): Conv2d(1, 16, kernel_size=(9, 9), stride=(1, 1), bias=False)\n",
       "    (conv1): PcConvBp(\n",
       "      (lambda_out): LambdaSingle()\n",
       "      (lambda_pred): LambdaSingle()\n",
       "      (FFconv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (act_fn): ReLU(inplace)\n",
       "    )\n",
       "    (fc): FactoredLinear2D()\n",
       "    (final_act): ReLU()\n",
       "    (pooling): AvgPool2d(kernel_size=3, stride=3, padding=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_v2.data.prepared.yuanyuan_8k import get_data\n",
    "from thesis_v2.training_extra.data import generate_datasets\n",
    "from thesis_v2.training.training import eval_wrapper\n",
    "\n",
    "from functools import partial\n",
    "from thesis_v2.training_extra.evaluation import eval_fn_wrapper as eval_fn_wrapper_neural\n",
    "import torch\n",
    "\n",
    "\n",
    "def eval_again(param, model, stats_best):\n",
    "    datasets = get_data('a', 200, param['input_size'], ('042318', '043018', '051018'), scale=0.5,\n",
    "                        seed=param['split_seed'])\n",
    "    datasets = {\n",
    "        'X_train': datasets[0].astype(np.float32),\n",
    "        'y_train': datasets[1],\n",
    "        'X_val': datasets[2].astype(np.float32),\n",
    "        'y_val': datasets[3],\n",
    "        'X_test': datasets[4].astype(np.float32),\n",
    "        'y_test': datasets[5],\n",
    "    }\n",
    "    \n",
    "    datasets_to_return = {\n",
    "        'X_test': datasets['X_test'],\n",
    "        'y_test': datasets['y_test'],\n",
    "    }\n",
    "    \n",
    "    # only the test one is needed.\n",
    "    datasets = generate_datasets(\n",
    "        **datasets,\n",
    "        per_epoch_train=True, shuffle_train=True,\n",
    "    )['test']\n",
    "    \n",
    "    result_on_the_go = eval_wrapper(model.cuda(),\n",
    "                                    datasets,\n",
    "                                    'cuda',\n",
    "                                    1,\n",
    "                                    partial(eval_fn_wrapper_neural, loss_type=param['loss_type']),\n",
    "                                        lambda dummy1,dummy2,dummy3: torch.tensor(0.0)\n",
    "                                   )\n",
    "    corrs = np.asarray(result_on_the_go['corr'])\n",
    "    corr_ref = np.asarray(stats_best['stats']['test']['corr'])\n",
    "    assert corrs.shape == corr_ref.shape == (79,)\n",
    "    \n",
    "    assert abs(corrs-corr_ref).max() < 1e-6\n",
    "    \n",
    "    return datasets_to_return\n",
    "    \n",
    "\n",
    "datasets_for_debug = eval_again(result['param'], result['result']['model'], result['result']['stats_best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_vars = {\n",
    "    'feature_file': join(dir_dict['features'],\n",
    "                            'maskcnn_polished_with_local_pcn',\n",
    "                            'debug.hdf5'\n",
    "                            ),\n",
    "    'augment_config': {\n",
    "        'module_names': ['bottomup', 'topdown', 'final'],\n",
    "        'name_mapping': {\n",
    "            'moduledict.conv1.lambda_out': 'bottomup',\n",
    "            'moduledict.conv1.lambda_pred': 'topdown',\n",
    "            'moduledict.final_act': 'final',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "from os import makedirs\n",
    "from os.path import dirname\n",
    "from thesis_v2.feature_extraction.extraction import extract_features\n",
    "import h5py\n",
    "from torch import tensor\n",
    "\n",
    "def augment_modules(model, datasets_to_extract, file_to_save, grp_name='features'):\n",
    "    augment_config = global_vars['augment_config']\n",
    "    makedirs(dirname(file_to_save), exist_ok=True)\n",
    "    with h5py.File(file_to_save) as f_feature:\n",
    "        if grp_name not in f_feature:\n",
    "            grp = f_feature.create_group(grp_name)\n",
    "\n",
    "            extract_features(model, (datasets_to_extract,),\n",
    "                             preprocessor=lambda x: (tensor(x[0]).cuda(),),\n",
    "                             output_group=grp,\n",
    "                             batch_size=50,\n",
    "                             augment_config=augment_config,\n",
    "                             # mostly for replicating old results\n",
    "                             deterministic=True\n",
    "                             )\n",
    "            \n",
    "augment_modules(result['result']['model'].eval(), datasets_for_debug['X_test'],\n",
    "                global_vars['feature_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to verify that these saved feature make sense.\n",
    "\n",
    "# let's check output `final` first. this is easy to check.\n",
    "def check_outputs(feature_file, corr_ref, y_data, loss_type, grp_name='features',):\n",
    "    with h5py.File(feature_file, 'r') as f_feature:\n",
    "        y_hat = f_feature[grp_name][str(global_vars['augment_config']['module_names'].index('final')) + '.0'][()]\n",
    "    assert y_hat.shape == y_data.shape == (1600, 79)\n",
    "    ret_dict = eval_fn_wrapper_neural(yhat_all=[[y_hat]], y_all=[[y_data]],loss_type=loss_type)\n",
    "    \n",
    "    corr = np.asarray(ret_dict['corr'])\n",
    "    assert corr_ref.shape == corr.shape == (79,)\n",
    "    \n",
    "    assert abs(corr-corr_ref).max() < 1e-6\n",
    "    \n",
    "check_outputs(global_vars['feature_file'], np.asarray(result['result']['stats_best']['stats']['test']['corr']),\n",
    "              datasets_for_debug['y_test'], result['param']['loss_type'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
