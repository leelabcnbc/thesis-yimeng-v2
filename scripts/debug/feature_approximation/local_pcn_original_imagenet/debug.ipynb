{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook sees how to create a feedforward approximator for recurrent features extracted in `scripts/feature_extraction/yuanyuan_8k_a/maskcnn_polished_with_local_pcn/debug.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common libs\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from sys import path\n",
    "from os.path import join\n",
    "\n",
    "from thesis_v2 import dir_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesis_v2.models.pcn_local.feature_extraction import get_one_network_meta\n",
    "from thesis_v2.models.pcn_local.reference.loader import get_pretrained_network\n",
    "\n",
    "meta = get_one_network_meta('PredNetBpE_3CLS')['module_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def get_layer_idx(friendly_name):\n",
    "    # friendly_name can be\n",
    "    # convX.in\n",
    "    # convX.init\n",
    "    # convX.loop\n",
    "    # X in 0~9.\n",
    "    return meta.index(friendly_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(feature_file, grp_name, conv_idx):\n",
    "    assert conv_idx in range(10) # 0 through 9.\n",
    "    slice_to_check = slice(None)\n",
    "    with h5py.File(feature_file, 'r') as f_feature:\n",
    "        grp = f_feature[grp_name]\n",
    "        assert str(get_layer_idx(f'conv{conv_idx}.init')) + '.0' in grp\n",
    "        num_bottom_up = 1 + len([x for x in grp if x.startswith(str(get_layer_idx(f'conv{conv_idx}.loop')) + '.')])\n",
    "        # should have at least two bottom up.\n",
    "        assert num_bottom_up > 1\n",
    "        \n",
    "        pcn_in = grp[str(get_layer_idx(f'conv{conv_idx}.in')) + '.0'][slice_to_check]\n",
    "        pcn_out_list = [grp[str(get_layer_idx(f'conv{conv_idx}.init'))+'.0'][slice_to_check]] + [grp[str(get_layer_idx(f'conv{conv_idx}.loop')) + f'.{x}'][slice_to_check] for x in range(num_bottom_up-1)]\n",
    "    \n",
    "    print((pcn_in.shape, pcn_in.mean(), pcn_in.std(), pcn_in.min(), pcn_in.max()))\n",
    "    print([(x.shape, x.mean(), x.std(), x.min(), x.max()) for x in pcn_out_list])\n",
    "    \n",
    "    return {\n",
    "        'in': pcn_in,\n",
    "        'out_list': pcn_out_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((500, 64, 112, 112), -0.023047889, 0.29318225, -6.253258, 7.049309)\n",
      "[((500, 64, 112, 112), 0.26036, 0.3671231, 0.0, 10.720739), ((500, 64, 112, 112), 0.3360843, 1.2205715, -7.905544, 15.050003), ((500, 64, 112, 112), 0.33592215, 1.2209634, -7.8925476, 15.168387), ((500, 64, 112, 112), 0.3357736, 1.2214642, -8.06066, 15.24714)]\n"
     ]
    }
   ],
   "source": [
    "data_returned = fetch_data(\n",
    "    join(dir_dict['features'], 'cnn_feature_extraction', 'imagenet_val', 'pcn_local.hdf5'),\n",
    "    'first500/PredNetBpE_3CLS/everything', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'PredNetBpE_3CLS'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredNetBpE(\n",
       "  (baseconv): features2(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (featBN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (PcConvs): ModuleList(\n",
       "    (0): PcConvBp(\n",
       "      (FFconv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (FBconv): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (b0): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 1x64x1x1 (GPU 1)])\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bypass): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (resp_init): Lambda()\n",
       "      (resp_loop): Lambda()\n",
       "    )\n",
       "    (1): PcConvBp(\n",
       "      (FFconv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (FBconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (b0): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 1x128x1x1 (GPU 1)])\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bypass): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (resp_init): Lambda()\n",
       "      (resp_loop): Lambda()\n",
       "    )\n",
       "    (2): PcConvBp(\n",
       "      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (FBconv): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (b0): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 1x128x1x1 (GPU 1)])\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (resp_init): Lambda()\n",
       "      (resp_loop): Lambda()\n",
       "    )\n",
       "    (3): PcConvBp(\n",
       "      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (FBconv): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (b0): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 1x128x1x1 (GPU 1)])\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (resp_init): Lambda()\n",
       "      (resp_loop): Lambda()\n",
       "    )\n",
       "    (4): PcConvBp(\n",
       "      (FFconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (FBconv): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (b0): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 1x128x1x1 (GPU 1)])\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bypass): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (resp_init): Lambda()\n",
       "      (resp_loop): Lambda()\n",
       "    )\n",
       "    (5): PcConvBp(\n",
       "      (FFconv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (FBconv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (b0): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 1x256x1x1 (GPU 1)])\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bypass): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (resp_init): Lambda()\n",
       "      (resp_loop): Lambda()\n",
       "    )\n",
       "    (6): PcConvBp(\n",
       "      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (FBconv): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (b0): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 1x256x1x1 (GPU 1)])\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (resp_init): Lambda()\n",
       "      (resp_loop): Lambda()\n",
       "    )\n",
       "    (7): PcConvBp(\n",
       "      (FFconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (FBconv): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (b0): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 1x256x1x1 (GPU 1)])\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bypass): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (resp_init): Lambda()\n",
       "      (resp_loop): Lambda()\n",
       "    )\n",
       "    (8): PcConvBp(\n",
       "      (FFconv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (FBconv): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (b0): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 1x512x1x1 (GPU 1)])\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bypass): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (resp_init): Lambda()\n",
       "      (resp_loop): Lambda()\n",
       "    )\n",
       "    (9): PcConvBp(\n",
       "      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (FBconv): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (b0): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 1x512x1x1 (GPU 1)])\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (resp_init): Lambda()\n",
       "      (resp_loop): Lambda()\n",
       "    )\n",
       "    (10): PcConvBp(\n",
       "      (FFconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (FBconv): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (b0): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 1x512x1x1 (GPU 1)])\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bypass): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (resp_init): Lambda()\n",
       "      (resp_loop): Lambda()\n",
       "    )\n",
       "  )\n",
       "  (BNs): ModuleList(\n",
       "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (maxpool_dict): ModuleDict(\n",
       "    (2): Lambda()\n",
       "    (4): Lambda()\n",
       "    (6): Lambda()\n",
       "    (9): Lambda()\n",
       "  )\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (BNend): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Lambda()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "model = get_pretrained_network(\n",
    "    'PredNetBpE_3CLS',\n",
    "    root_dir=join(\n",
    "        dir_dict['root'], '..', 'thesis-yimeng-v1', '3rdparty',\n",
    "        'PCN-with-Local-Recurrent-Processing', 'checkpoint'\n",
    "    )\n",
    ")\n",
    "# do not use main GPU, as the model takes too much memory, leaving no space for training the approximator.\n",
    "model.cuda(device=1)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "0.07572497 1.2192386 -7.905544 14.768319\n",
      "0.0\n",
      "0.0\n",
      "(0, 2)\n",
      "0.07556095 1.2197504 -7.8925476 14.776471\n",
      "0.0\n",
      "0.0\n",
      "(0, 3)\n",
      "0.07541336 1.2203431 -8.06066 14.813073\n",
      "0.0\n",
      "0.0\n",
      "(1, 2)\n",
      "-0.00016407052 0.013022245 -2.3945658 1.9086058\n",
      "0.0\n",
      "0.0\n",
      "(1, 3)\n",
      "-0.0003115602 0.02519667 -4.4206944 3.374986\n",
      "0.0\n",
      "0.0\n",
      "(2, 3)\n",
      "-0.00014748999 0.012218872 -2.0261285 1.4663801\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# # the idea is, given idx1 and idx2, predict out_list[idx2] - out_list[idx1]  given (out_list[idx1]  and in).\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from torch.backends import cudnn\n",
    "import torch\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "import torch.nn.functional as F\n",
    "def check_similarity(d1, d2):\n",
    "    assert d1.shape == d2.shape\n",
    "    norm_diff = norm(d1-d2)/norm(d2)\n",
    "    print(norm_diff)\n",
    "    print(abs(d1-d2).max())\n",
    "    assert norm_diff < 1e-5\n",
    "\n",
    "def debug_result(model_,in_,out1,idx_diff):\n",
    "    # model_ is PcConvBp module.\n",
    "    assert idx_diff > 0\n",
    "    \n",
    "    #  y = self.relu(self.FFconv(x))\n",
    "#         y = self.resp_init(y)\n",
    "#         b0 = F.relu(self.b0[0]+1.0).expand_as(y)\n",
    "#         for _ in range(self.cls):\n",
    "#             y = self.FFconv(self.relu(x - self.FBconv(y)))*b0 + y\n",
    "#             y = self.resp_loop(y)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(in_).cuda(device=1)\n",
    "        y = torch.tensor(out1).cuda(device=1)\n",
    "        b0 = F.relu(model_.b0[0]+1.0).expand_as(y)\n",
    "        for _ in range(idx_diff):\n",
    "            y = model_.FFconv(model_.relu(x - model_.FBconv(y)))*b0 + y\n",
    "            y = model_.resp_loop(y)\n",
    "        \n",
    "    return y.cpu().numpy() - out1\n",
    "\n",
    "def check_result(model_, data_dict):\n",
    "    num_out = len(data_dict['out_list'])\n",
    "    \n",
    "    for idx1 in range(num_out):\n",
    "        for idx2 in range(idx1+1, num_out):\n",
    "            print((idx1, idx2))\n",
    "            result_ref = data_dict['out_list'][idx2] - data_dict['out_list'][idx1]\n",
    "            print(result_ref.mean(), result_ref.std(), result_ref.min(), result_ref.max())\n",
    "            result_debug = debug_result(model_,data_dict['in'],data_dict['out_list'][idx1],idx2-idx1)\n",
    "            check_similarity(result_ref, result_debug)\n",
    "\n",
    "# all ok.\n",
    "check_result(model.PcConvs[0], data_returned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now time to get a model to train it.\n",
    "# simple stuff. conv + relu.\n",
    "# maybe with BN.\n",
    "\n",
    "# two kinds of models\n",
    "\n",
    "# BN + conv + ReLU + BN\n",
    "# conv + ReLU + BN\n",
    "# I may want to constrain the first BN a bit,\n",
    "# say, all in_ channels share the same scale and bias; same goes with out1 channels.\n",
    "\n",
    "# some concerns: stats are different for `out_` at different iterations.\n",
    "# but let's ignore it for now.\n",
    "from thesis_v2.models.feature_approximation.builder import (\n",
    "    gen_local_pcn_recurrent_feature_approximator\n",
    ")\n",
    "\n",
    "from thesis_v2.training_extra.feature_approximation.opt import get_feature_approximation_opt_config\n",
    "from thesis_v2.training_extra.feature_approximation.training import train_one\n",
    "\n",
    "def handle_one_case(*,\n",
    "                    data_dict,\n",
    "                    kernel_size,\n",
    "                    note,\n",
    "                    batchnorm_pre=True,\n",
    "                    batchnorm_post=True,\n",
    "                    act_fn='relu',\n",
    "                   ):\n",
    "    \n",
    "    # prepare dataset\n",
    "    num_out = len(data_dict['out_list'])\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for idx1 in range(num_out):\n",
    "        for idx2 in range(idx1+1, num_out):\n",
    "            if idx2 - idx1 != 2:\n",
    "                continue\n",
    "            print((idx1, idx2))\n",
    "            x_train.append(np.concatenate([data_dict['in'],data_dict['out_list'][idx1]], axis=1))\n",
    "            y_train.append(data_dict['out_list'][idx2] - data_dict['out_list'][idx1])\n",
    "    \n",
    "    x_train = np.concatenate(x_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    \n",
    "    print((x_train.shape, y_train.shape))\n",
    "    \n",
    "    dataset_this = {\n",
    "        'X_train': x_train,\n",
    "        'y_train': y_train,\n",
    "    }\n",
    "    \n",
    "    def gen_cnn_partial(in_shape):\n",
    "        # I assume two inputs have the same number of channels and shapes.\n",
    "        assert len(in_shape) == 3\n",
    "        assert in_shape[0] % 2 == 0\n",
    "        return gen_local_pcn_recurrent_feature_approximator(\n",
    "            in_shape_lower=[in_shape[0]//2, in_shape[1], in_shape[2]],\n",
    "            in_shape_higher=[in_shape[0]//2, in_shape[1], in_shape[2]],\n",
    "            kernel_size=kernel_size,\n",
    "            act_fn=act_fn,\n",
    "        )\n",
    "    #\n",
    "    res = train_one(arch_json_partial=gen_cnn_partial,\n",
    "                    opt_config_partial=get_feature_approximation_opt_config,\n",
    "                    datasets=dataset_this,\n",
    "                    # note this gets saved under v1 folder...\n",
    "                    # but it should not matter.\n",
    "                    key=f'debug/feature_approximation/local_pcn_original_imagenet_feature_approximator/note{note}/kernel_size{kernel_size}/act_fn{act_fn}/batchnorm_pre{batchnorm_pre}/batchnorm_post{batchnorm_post}',\n",
    "                    show_every=100,\n",
    "                    max_epoch=10000,\n",
    "                    # set max_epoch to 7000 for other models, so that I won't wait too long and I suppose 7000 should be long enough.\n",
    "                    model_seed=0, return_model=False,\n",
    "                    # default 256 is too big.\n",
    "                    batch_size=32,\n",
    "                   )\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2)\n",
      "(1, 3)\n",
      "((1000, 128, 112, 112), (1000, 64, 112, 112))\n",
      "num_param 664000\n",
      "========starting phase 1/3==========\n",
      "========starting epoch 0==========\n",
      "0-0, train loss 1.9601954221725464\n",
      "train loss 1.9601954221725464\n",
      "========done epoch 0==========\n",
      "========starting epoch 100==========\n",
      "100-0, train loss 0.8618829846382141\n",
      "train loss 0.8618829846382141\n",
      "========done epoch 100==========\n",
      "========starting epoch 200==========\n",
      "200-0, train loss 0.7272781133651733\n",
      "train loss 0.7272781133651733\n",
      "========done epoch 200==========\n",
      "========starting epoch 300==========\n",
      "300-0, train loss 0.3992619812488556\n",
      "train loss 0.3992619812488556\n",
      "========done epoch 300==========\n",
      "========starting epoch 400==========\n",
      "400-0, train loss 0.3236750662326813\n",
      "train loss 0.3236750662326813\n",
      "========done epoch 400==========\n",
      "========starting epoch 500==========\n",
      "500-0, train loss 0.46670547127723694\n",
      "train loss 0.46670547127723694\n",
      "========done epoch 500==========\n",
      "========starting epoch 600==========\n",
      "600-0, train loss 0.20256838202476501\n",
      "train loss 0.20256838202476501\n",
      "========done epoch 600==========\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-461994338785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_returned\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'debug'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-9-51d02846f82a>\u001b[0m in \u001b[0;36mhandle_one_case\u001b[0;34m(data_dict, kernel_size, note, batchnorm_pre, batchnorm_post, act_fn)\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0mmodel_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0;31m# default 256 is too big.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                    )\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training_extra/feature_approximation/training.py\u001b[0m in \u001b[0;36mtrain_one\u001b[0;34m(arch_json_partial, opt_config_partial, datasets, key, show_every, model_seed, train_seed, max_epoch, device, return_model, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mreturn_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         extra_params={\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;34m'datasets'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'y_dim'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         },\n\u001b[1;32m     54\u001b[0m     )\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training_extra/training.py\u001b[0m in \u001b[0;36mtrain_one_wrapper\u001b[0;34m(get_json_fn, initialize_model_fn, get_optimizer_fn, get_loss_fn, datasets, key, show_every, model_seed, train_seed, max_epoch, early_stopping_field, device, val_test_every, return_model, extra_params)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mextra_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     )\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training_extra/training.py\u001b[0m in \u001b[0;36mtrain_one_inner\u001b[0;34m(model, datasets, key, optimizer, loss_fn, config_extra, seed, config, eval_fn, return_model, extra_params, shuffle_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m                             \u001b[0meval_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                             legacy_random_seed=True)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training_aux.py\u001b[0m in \u001b[0;36mtraining_wrapper\u001b[0;34m(model, loss_fn, optimizer, dataset_train, dataset_val, dataset_test, config, config_extra, eval_fn, key, return_model, deterministic, legacy_random_seed)\u001b[0m\n\u001b[1;32m    190\u001b[0m                            \u001b[0meval_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_normed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                            \u001b[0mf_best\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_best_tmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                            legacy_random_seed=legacy_random_seed)\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;31m# then copy/move that file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_best_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_func, optimizer, config, f_best, dataset_train, dataset_val, dataset_test, eval_fn, legacy_random_seed)\u001b[0m\n\u001b[1;32m    123\u001b[0m                                                        \u001b[0mstats_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                                                        \u001b[0mf_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_phase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                                                        legacy_random_seed)\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mstats_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats_this_phase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'========end phase {i_phase + 1}/{num_phase}=========='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training.py\u001b[0m in \u001b[0;36mtrain_one_phase\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                             \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                                             \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                                             dataset_train, print_flag)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         stats_this_batch['val'] = _val_test_one_epoch(i_epoch, model,\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training.py\u001b[0m in \u001b[0;36m_train_one_epoch\u001b[0;34m(i_epoch, model, optimizer, loss_func, conf, dataset_train, print_flag)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_minibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_this_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;31m# double check that I'm training properly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# in other words, I'm not doing dropout improperly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training_aux.py\u001b[0m in \u001b[0;36mgenerate_new_n_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_new_n_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training_aux.py\u001b[0m in \u001b[0;36mcycle_reboot\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# cycle('ABCD') --> A B C D A B C D A B C D ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "handle_one_case(\n",
    "    data_dict=data_returned,\n",
    "    kernel_size=9,\n",
    "    note='debug',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
