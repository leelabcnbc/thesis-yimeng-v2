{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook sees how to create a feedforward approximator for recurrent features extracted in `scripts/feature_extraction/yuanyuan_8k_a/maskcnn_polished_with_rcnn_k_bl/20200218.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common libs\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from sys import path\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "from thesis_v2 import dir_dict\n",
    "\n",
    "from torchnetjson.builder import build_net\n",
    "from thesis_v2.training.training_aux import load_training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from that file.\n",
    "from thesis_v2.models.maskcnn_polished_with_rcnn_k_bl.builder import load_modules\n",
    "\n",
    "global_vars = {\n",
    "    'feature_file_dir': join(dir_dict['features'],\n",
    "                             'maskcnn_polished_with_rcnn_k_bl',\n",
    "                             '20200218'),\n",
    "    'augment_config': {\n",
    "        'module_names': ['layer0', 'layer1', 'layer2'],\n",
    "        'name_mapping': {\n",
    "            'moduledict.bl_stack.input_capture': 'layer0',\n",
    "            'moduledict.bl_stack.capture_list.0': 'layer1',\n",
    "            'moduledict.bl_stack.capture_list.1': 'layer2',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "from thesis_v2.configs.model.maskcnn_polished_with_rcnn_k_bl import (\n",
    "    explored_models_20200218,\n",
    "    script_keygen,\n",
    "    keygen\n",
    ")\n",
    "\n",
    "load_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_file_dir': '/my_data/thesis-yimeng-v2/results/features/maskcnn_polished_with_rcnn_k_bl/20200218', 'augment_config': {'module_names': ['layer0', 'layer1', 'layer2'], 'name_mapping': {'moduledict.bl_stack.input_capture': 'layer0', 'moduledict.bl_stack.capture_list.0': 'layer1', 'moduledict.bl_stack.capture_list.1': 'layer2'}}}\n"
     ]
    }
   ],
   "source": [
    "print(global_vars)\n",
    "\n",
    "# utils\n",
    "def get_layer_idx(friendly_name):\n",
    "    return global_vars['augment_config']['module_names'].index(friendly_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_model_param(param):\n",
    "    # test maximal number of channel in case of out of memory.\n",
    "    return param['rcnn_bl_cls'] == 4 and param['kernel_size_l23'] == 3 and param['num_layer'] == 3 and param['out_channel'] == 32\n",
    "\n",
    "\n",
    "def get_all_model_params():\n",
    "    all_params_dict = dict()\n",
    "    for idx, param in enumerate(explored_models_20200218().generate()):\n",
    "        # let's use a fully recurrent one for debugging.\n",
    "        if not good_model_param(param):\n",
    "            continue\n",
    "\n",
    "        key = keygen(**{k: v for k, v in param.items() if k not in {'scale', 'smoothness'}})\n",
    "        key_script = script_keygen(**param)\n",
    "        all_params_dict[key_script] = {\n",
    "            'key': key,\n",
    "            'param': param,\n",
    "        }\n",
    "\n",
    "    return all_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all model params\n",
    "all_params_dict = get_all_model_params()\n",
    "\n",
    "model_to_check = 's_selegacy+in_sz50+out_ch32+num_l3+k_l19+k_p3+ptavg+bn_a_fcFalse+actrelu+r_c4+r_psize1+r_ptypeNone+r_acccummean+ff1st_True+ff1stbba_True+sc0.01+sm0.000005+lmse+m_se0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract input 'topdown.0' as well as all 'bottomup.*' except the last one.\n",
    "\n",
    "def fetch_data(feature_file, grp_name):\n",
    "    slice_to_check = slice(None)\n",
    "    with h5py.File(feature_file, 'r') as f_feature:\n",
    "        grp = f_feature[grp_name]\n",
    "        num_bottom_up = len([x for x in grp if x.startswith(str(get_layer_idx('layer2')) + '.')])\n",
    "        assert num_bottom_up > 1\n",
    "        assert num_bottom_up == len([x for x in grp if x.startswith(str(get_layer_idx('layer1')) + '.')])\n",
    "        \n",
    "        pcn_in = grp[str(get_layer_idx('layer0')) + '.0'][slice_to_check]\n",
    "        pcn_inter_list = [grp[str(get_layer_idx('layer1')) + f'.{x}'][slice_to_check] for x in range(num_bottom_up)]\n",
    "        pcn_out_list = [grp[str(get_layer_idx('layer2')) + f'.{x}'][slice_to_check] for x in range(num_bottom_up)]\n",
    "    \n",
    "    print((pcn_in.shape, pcn_in.mean(), pcn_in.std(), pcn_in.min(), pcn_in.max()))\n",
    "    print([(x.shape, x.mean(), x.std(), x.min(), x.max()) for x in pcn_inter_list])\n",
    "    print([(x.shape, x.mean(), x.std(), x.min(), x.max()) for x in pcn_out_list])\n",
    "    \n",
    "    return {\n",
    "        'in': pcn_in,\n",
    "        'inter_list': pcn_inter_list,\n",
    "        'out_list': pcn_out_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((5120, 32, 42, 42), 0.00097621273, 0.13129921, -1.5368805, 1.6458784)\n",
      "[((5120, 32, 42, 42), 0.30902508, 0.5585249, 0.0, 13.410167), ((5120, 32, 42, 42), 0.38653353, 0.5509103, 0.0, 10.5434675), ((5120, 32, 42, 42), 0.38611388, 0.5902634, 0.0, 10.2964525), ((5120, 32, 42, 42), 0.38038015, 0.58427274, 0.0, 11.386213)]\n",
      "[((5120, 32, 42, 42), 0.8971239, 1.1501049, 0.0, 28.444695), ((5120, 32, 42, 42), 0.9062065, 1.3005654, 0.0, 23.221088), ((5120, 32, 42, 42), 0.87318933, 1.303688, 0.0, 23.964273), ((5120, 32, 42, 42), 0.8348414, 1.2768589, 0.0, 22.642118)]\n"
     ]
    }
   ],
   "source": [
    "data_returned = fetch_data(join(global_vars['feature_file_dir'], model_to_check + '.hdf5'), 'X_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "def load_model(key):\n",
    "    result = load_training_results(key, return_model=False)\n",
    "    # load twice, first time to get the model.\n",
    "    model_ = load_training_results(key, return_model=True, model=build_net(result['config_extra']['model']))['model']\n",
    "    model_.cuda()\n",
    "    model_.eval()\n",
    "    return model_\n",
    "\n",
    "model = load_model(all_params_dict[model_to_check]['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "0.9064885 1.2906708 0.0 19.358885\n",
      "(32, 32, 42, 42) (32, 32, 42, 42) (32, 32, 42, 42)\n",
      "0.0\n",
      "0.0\n",
      "(0, 2)\n",
      "0.87911874 1.2874544 0.0 19.469995\n",
      "(32, 32, 42, 42) (32, 32, 42, 42) (32, 32, 42, 42)\n",
      "0.0\n",
      "0.0\n",
      "(0, 3)\n",
      "0.844596 1.2629777 0.0 17.920813\n",
      "(32, 32, 42, 42) (32, 32, 42, 42) (32, 32, 42, 42)\n",
      "0.0\n",
      "0.0\n",
      "(1, 2)\n",
      "0.87911874 1.2874544 0.0 19.469995\n",
      "(32, 32, 42, 42) (32, 32, 42, 42) (32, 32, 42, 42)\n",
      "0.0\n",
      "0.0\n",
      "(1, 3)\n",
      "0.844596 1.2629777 0.0 17.920813\n",
      "(32, 32, 42, 42) (32, 32, 42, 42) (32, 32, 42, 42)\n",
      "0.0\n",
      "0.0\n",
      "(2, 3)\n",
      "0.844596 1.2629777 0.0 17.920813\n",
      "(32, 32, 42, 42) (32, 32, 42, 42) (32, 32, 42, 42)\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# # the idea is, given idx1 and idx2, predict out_list[idx2] - out_list[idx1]  given (out_list[idx1]  and in).\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from torch.backends import cudnn\n",
    "import torch\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "def check_similarity(d1, d2):\n",
    "    assert d1.shape == d2.shape\n",
    "    norm_diff = norm(d1-d2)/norm(d2)\n",
    "    print(norm_diff)\n",
    "    print(abs(d1-d2).max())\n",
    "    assert norm_diff < 1e-5\n",
    "\n",
    "def debug_result(model_,\n",
    "                 in_,\n",
    "                 inter_,\n",
    "                 out_,\n",
    "                 idx_diff,\n",
    "                 # this determines which BN layer to use.\n",
    "                 time_start,\n",
    "                ):\n",
    "    assert idx_diff > 0\n",
    "    assert time_start >= 0\n",
    "    \n",
    "    print(in_.shape, inter_.shape, out_.shape)\n",
    "    \n",
    "    model_ = model_.moduledict['bl_stack']\n",
    "    \n",
    "    assert model_.n_layer == 2\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        last_out = [torch.tensor(inter_).cuda(), torch.tensor(out_).cuda()]\n",
    "        b_input = torch.tensor(in_).cuda()\n",
    "        for t in range(time_start, time_start + idx_diff):\n",
    "            for layer_idx in range(model_.n_layer):\n",
    "                layer_this = model_.layer_list[layer_idx]\n",
    "                bn_this = model_.bn_layer_list[t * model_.n_layer + layer_idx]\n",
    "                if layer_idx == 0:\n",
    "                    last_out[layer_idx] = layer_this(b_input, last_out[layer_idx])\n",
    "                else:\n",
    "                    last_out[layer_idx] = layer_this(last_out[layer_idx - 1], last_out[layer_idx])\n",
    "                # do batch norm\n",
    "                last_out[layer_idx] = bn_this(last_out[layer_idx])\n",
    "                # do act\n",
    "                last_out[layer_idx] = model_.act_fn(last_out[layer_idx])\n",
    "    return last_out[-1].cpu().numpy()\n",
    "\n",
    "def check_result(model_, data_dict):\n",
    "    num_out = len(data_dict['out_list'])\n",
    "    \n",
    "    # only start from the beginning. otherwise we need some buffer to calculate.\n",
    "    for idx1 in range(num_out):\n",
    "        for idx2 in range(idx1+1, num_out):\n",
    "            print((idx1, idx2))\n",
    "            result_ref = data_dict['out_list'][idx2][:32]\n",
    "            print(result_ref.mean(), result_ref.std(), result_ref.min(), result_ref.max())\n",
    "            result_debug = debug_result(model_,data_dict['in'][:32],\n",
    "                                        data_dict['inter_list'][idx1][:32],\n",
    "                                        data_dict['out_list'][idx1][:32],\n",
    "                                        idx2-idx1,\n",
    "                                        idx1+1)\n",
    "            check_similarity(result_ref, result_debug)\n",
    "\n",
    "# all ok.\n",
    "check_result(model, data_returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now time to get a model to train it.\n",
    "# simple stuff. conv + relu.\n",
    "# maybe with BN.\n",
    "\n",
    "# two kinds of models\n",
    "\n",
    "# BN + conv + ReLU + BN\n",
    "# conv + ReLU + BN\n",
    "# I may want to constrain the first BN a bit,\n",
    "# say, all in_ channels share the same scale and bias; same goes with out1 channels.\n",
    "\n",
    "# some concerns: stats are different for `out_` at different iterations.\n",
    "# but let's ignore it for now.\n",
    "from thesis_v2.models.feature_approximation.builder import (\n",
    "    gen_local_pcn_recurrent_feature_approximator\n",
    ")\n",
    "\n",
    "from thesis_v2.training_extra.feature_approximation.opt import get_feature_approximation_opt_config\n",
    "from thesis_v2.training_extra.feature_approximation.training import train_one\n",
    "\n",
    "def handle_one_case(*,\n",
    "                    data_dict,\n",
    "                    kernel_size,\n",
    "                    note,\n",
    "                    batchnorm_pre=True,\n",
    "                    batchnorm_post=True,\n",
    "                    act_fn='relu',\n",
    "                   ):\n",
    "    \n",
    "    # prepare dataset\n",
    "    num_out = len(data_dict['out_list'])\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for idx1 in range(0, num_out-1):\n",
    "        # we should use a fixed idx1, as the function to be modeled depends not only on idx1,\n",
    "        # but also on idx2 - idx1.\n",
    "        for idx2 in range(idx1+1, num_out):\n",
    "            if idx2 - idx1 != 2:\n",
    "                continue\n",
    "            print((idx1, idx2))\n",
    "            x_train.append(np.concatenate([data_dict['in'],data_dict['inter_list'][idx1],data_dict['out_list'][idx1]], axis=1))\n",
    "            # using the difference (`data_dict['out_list'][idx2] - data_dict['out_list'][idx1]`)\n",
    "            #    or data_dict['out_list'][idx2] makes little difference.\n",
    "            y_train.append(data_dict['out_list'][idx2])\n",
    "        break\n",
    "    \n",
    "    x_train = np.concatenate(x_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    \n",
    "    print((x_train.shape, y_train.shape))\n",
    "    print(y_train.mean(), y_train.std())\n",
    "    \n",
    "    dataset_this = {\n",
    "        'X_train': x_train,\n",
    "        'y_train': y_train,\n",
    "    }\n",
    "    \n",
    "    def gen_cnn_partial(in_shape, in_y_shape):\n",
    "        assert len(in_shape) == 3\n",
    "        assert len(in_y_shape) == 3\n",
    "        in_higher_c = in_y_shape[0]\n",
    "        in_lower_c = in_shape[0] - in_higher_c\n",
    "        \n",
    "        return gen_local_pcn_recurrent_feature_approximator(\n",
    "            in_shape_lower=[in_lower_c, in_shape[1], in_shape[2]],\n",
    "            in_shape_higher=[in_higher_c, in_shape[1], in_shape[2]],\n",
    "            kernel_size=kernel_size,\n",
    "            act_fn=act_fn,\n",
    "            # modeling non residual\n",
    "            bn_before_act=True,\n",
    "        )\n",
    "    #\n",
    "    res = train_one(arch_json_partial=gen_cnn_partial,\n",
    "                    opt_config_partial=get_feature_approximation_opt_config,\n",
    "                    datasets=dataset_this,\n",
    "                    # note this gets saved under v1 folder...\n",
    "                    # but it should not matter.\n",
    "                    key=f'debug/feature_approximation/k_bl_ksize3_nonresidual/note{note}/kernel_size{kernel_size}/act_fn{act_fn}/batchnorm_pre{batchnorm_pre}/batchnorm_post{batchnorm_post}',\n",
    "                    show_every=50,\n",
    "                    model_seed=0, return_model=False)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2)\n",
      "((5120, 96, 42, 42), (5120, 32, 42, 42))\n",
      "0.87318933 1.303688\n",
      "num_param 249088\n",
      "num of phase:  3\n",
      "========starting phase 1/3==========\n",
      "========starting epoch 0==========\n",
      "0-0, train loss 2.2811152935028076\n",
      "train loss 2.2811152935028076\n",
      "========done epoch 0==========\n",
      "========starting epoch 50==========\n",
      "50-0, train loss 0.8710591793060303\n",
      "train loss 0.8710591793060303\n",
      "========done epoch 50==========\n",
      "========starting epoch 100==========\n",
      "100-0, train loss 0.6269522905349731\n",
      "train loss 0.6269522905349731\n",
      "========done epoch 100==========\n",
      "========starting epoch 150==========\n",
      "150-0, train loss 0.528144359588623\n",
      "train loss 0.528144359588623\n",
      "========done epoch 150==========\n",
      "========starting epoch 200==========\n",
      "200-0, train loss 0.43887272477149963\n",
      "train loss 0.43887272477149963\n",
      "========done epoch 200==========\n",
      "========starting epoch 250==========\n",
      "250-0, train loss 0.35570067167282104\n",
      "train loss 0.35570067167282104\n",
      "========done epoch 250==========\n",
      "========starting epoch 300==========\n",
      "300-0, train loss 0.30795472860336304\n",
      "train loss 0.30795472860336304\n",
      "========done epoch 300==========\n",
      "========starting epoch 350==========\n",
      "350-0, train loss 0.27244260907173157\n",
      "train loss 0.27244260907173157\n",
      "========done epoch 350==========\n",
      "========starting epoch 400==========\n",
      "400-0, train loss 0.24446451663970947\n",
      "train loss 0.24446451663970947\n",
      "========done epoch 400==========\n",
      "========starting epoch 450==========\n",
      "450-0, train loss 0.2127188742160797\n",
      "train loss 0.2127188742160797\n",
      "========done epoch 450==========\n",
      "========starting epoch 500==========\n",
      "500-0, train loss 0.1862032264471054\n",
      "train loss 0.1862032264471054\n",
      "========done epoch 500==========\n",
      "========starting epoch 550==========\n",
      "550-0, train loss 0.1792534738779068\n",
      "train loss 0.1792534738779068\n",
      "========done epoch 550==========\n",
      "========starting epoch 600==========\n",
      "600-0, train loss 0.15911610424518585\n",
      "train loss 0.15911610424518585\n",
      "========done epoch 600==========\n",
      "========starting epoch 650==========\n",
      "650-0, train loss 0.14789623022079468\n",
      "train loss 0.14789623022079468\n",
      "========done epoch 650==========\n",
      "========starting epoch 700==========\n",
      "700-0, train loss 0.14482124149799347\n",
      "train loss 0.14482124149799347\n",
      "========done epoch 700==========\n",
      "========starting epoch 750==========\n",
      "750-0, train loss 0.13291892409324646\n",
      "train loss 0.13291892409324646\n",
      "========done epoch 750==========\n",
      "========starting epoch 800==========\n",
      "800-0, train loss 0.13388656079769135\n",
      "train loss 0.13388656079769135\n",
      "========done epoch 800==========\n",
      "========starting epoch 850==========\n",
      "850-0, train loss 0.12640546262264252\n",
      "train loss 0.12640546262264252\n",
      "========done epoch 850==========\n",
      "========starting epoch 900==========\n",
      "900-0, train loss 0.12323632091283798\n",
      "train loss 0.12323632091283798\n",
      "========done epoch 900==========\n",
      "========starting epoch 950==========\n",
      "950-0, train loss 0.11730214953422546\n",
      "train loss 0.11730214953422546\n",
      "========done epoch 950==========\n",
      "========starting epoch 1000==========\n",
      "1000-0, train loss 0.11601846665143967\n",
      "train loss 0.11601846665143967\n",
      "========done epoch 1000==========\n",
      "========starting epoch 1050==========\n",
      "1050-0, train loss 0.11608951538801193\n",
      "train loss 0.11608951538801193\n",
      "========done epoch 1050==========\n",
      "========starting epoch 1100==========\n",
      "1100-0, train loss 0.11328763514757156\n",
      "train loss 0.11328763514757156\n",
      "========done epoch 1100==========\n",
      "========starting epoch 1150==========\n",
      "1150-0, train loss 0.11769653856754303\n",
      "train loss 0.11769653856754303\n",
      "========done epoch 1150==========\n",
      "========starting epoch 1200==========\n",
      "1200-0, train loss 0.10966414213180542\n",
      "train loss 0.10966414213180542\n",
      "========done epoch 1200==========\n",
      "========starting epoch 1250==========\n",
      "1250-0, train loss 0.1097560003399849\n",
      "train loss 0.1097560003399849\n",
      "========done epoch 1250==========\n",
      "========starting epoch 1300==========\n",
      "1300-0, train loss 0.11046972125768661\n",
      "train loss 0.11046972125768661\n",
      "========done epoch 1300==========\n",
      "========starting epoch 1350==========\n",
      "1350-0, train loss 0.10886190086603165\n",
      "train loss 0.10886190086603165\n",
      "========done epoch 1350==========\n",
      "========starting epoch 1400==========\n",
      "1400-0, train loss 0.1080159917473793\n",
      "train loss 0.1080159917473793\n",
      "========done epoch 1400==========\n",
      "========starting epoch 1450==========\n",
      "1450-0, train loss 0.1050211489200592\n",
      "train loss 0.1050211489200592\n",
      "========done epoch 1450==========\n",
      "========starting epoch 1500==========\n",
      "1500-0, train loss 0.10674206912517548\n",
      "train loss 0.10674206912517548\n",
      "========done epoch 1500==========\n",
      "========starting epoch 1550==========\n",
      "1550-0, train loss 0.10704363882541656\n",
      "train loss 0.10704363882541656\n",
      "========done epoch 1550==========\n",
      "========starting epoch 1600==========\n",
      "1600-0, train loss 0.10797598958015442\n",
      "train loss 0.10797598958015442\n",
      "========done epoch 1600==========\n",
      "========starting epoch 1650==========\n",
      "1650-0, train loss 0.10715115070343018\n",
      "train loss 0.10715115070343018\n",
      "========done epoch 1650==========\n",
      "========starting epoch 1700==========\n",
      "1700-0, train loss 0.10834567248821259\n",
      "train loss 0.10834567248821259\n",
      "========done epoch 1700==========\n",
      "========starting epoch 1750==========\n",
      "1750-0, train loss 0.10862549394369125\n",
      "train loss 0.10862549394369125\n",
      "========done epoch 1750==========\n",
      "========starting epoch 1800==========\n",
      "1800-0, train loss 0.10929709672927856\n",
      "train loss 0.10929709672927856\n",
      "========done epoch 1800==========\n",
      "========starting epoch 1850==========\n",
      "1850-0, train loss 0.11103539913892746\n",
      "train loss 0.11103539913892746\n",
      "========done epoch 1850==========\n",
      "========starting epoch 1900==========\n",
      "1900-0, train loss 0.10801881551742554\n",
      "train loss 0.10801881551742554\n",
      "========done epoch 1900==========\n",
      "========starting epoch 1950==========\n",
      "1950-0, train loss 0.10661016404628754\n",
      "train loss 0.10661016404628754\n",
      "========done epoch 1950==========\n",
      "========starting epoch 2000==========\n",
      "2000-0, train loss 0.10359805077314377\n",
      "train loss 0.10359805077314377\n",
      "========done epoch 2000==========\n",
      "========starting epoch 2050==========\n",
      "2050-0, train loss 0.10804581642150879\n",
      "train loss 0.10804581642150879\n",
      "========done epoch 2050==========\n",
      "========starting epoch 2100==========\n",
      "2100-0, train loss 0.10473228991031647\n",
      "train loss 0.10473228991031647\n",
      "========done epoch 2100==========\n",
      "========starting epoch 2150==========\n",
      "2150-0, train loss 0.10472375899553299\n",
      "train loss 0.10472375899553299\n",
      "========done epoch 2150==========\n",
      "========starting epoch 2200==========\n",
      "2200-0, train loss 0.10632593184709549\n",
      "train loss 0.10632593184709549\n",
      "========done epoch 2200==========\n",
      "========starting epoch 2250==========\n",
      "2250-0, train loss 0.10406742990016937\n",
      "train loss 0.10406742990016937\n",
      "========done epoch 2250==========\n",
      "========starting epoch 2300==========\n",
      "2300-0, train loss 0.10790791362524033\n",
      "train loss 0.10790791362524033\n",
      "========done epoch 2300==========\n",
      "========starting epoch 2350==========\n",
      "2350-0, train loss 0.1078244000673294\n",
      "train loss 0.1078244000673294\n",
      "========done epoch 2350==========\n",
      "========starting epoch 2400==========\n",
      "2400-0, train loss 0.10410493612289429\n",
      "train loss 0.10410493612289429\n",
      "========done epoch 2400==========\n",
      "========starting epoch 2450==========\n",
      "2450-0, train loss 0.10718376934528351\n",
      "train loss 0.10718376934528351\n",
      "========done epoch 2450==========\n",
      "========starting epoch 2500==========\n",
      "2500-0, train loss 0.10160480439662933\n",
      "train loss 0.10160480439662933\n",
      "========done epoch 2500==========\n",
      "========starting epoch 2550==========\n",
      "2550-0, train loss 0.10591765493154526\n",
      "train loss 0.10591765493154526\n",
      "========done epoch 2550==========\n",
      "========starting epoch 2600==========\n",
      "2600-0, train loss 0.10365089774131775\n",
      "train loss 0.10365089774131775\n",
      "========done epoch 2600==========\n",
      "========starting epoch 2650==========\n",
      "2650-0, train loss 0.10511964559555054\n",
      "train loss 0.10511964559555054\n",
      "========done epoch 2650==========\n",
      "========starting epoch 2700==========\n",
      "2700-0, train loss 0.10301169008016586\n",
      "train loss 0.10301169008016586\n",
      "========done epoch 2700==========\n",
      "========starting epoch 2750==========\n",
      "2750-0, train loss 0.10355925559997559\n",
      "train loss 0.10355925559997559\n",
      "========done epoch 2750==========\n",
      "========starting epoch 2800==========\n",
      "2800-0, train loss 0.10330177843570709\n",
      "train loss 0.10330177843570709\n",
      "========done epoch 2800==========\n",
      "========starting epoch 2850==========\n",
      "2850-0, train loss 0.10509373247623444\n",
      "train loss 0.10509373247623444\n",
      "========done epoch 2850==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========starting epoch 2900==========\n",
      "2900-0, train loss 0.10095887631177902\n",
      "train loss 0.10095887631177902\n",
      "========done epoch 2900==========\n",
      "========starting epoch 2950==========\n",
      "2950-0, train loss 0.10094667226076126\n",
      "train loss 0.10094667226076126\n",
      "========done epoch 2950==========\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-86651fd585bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_returned\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'debug13'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-32cf6fe6bebb>\u001b[0m in \u001b[0;36mhandle_one_case\u001b[0;34m(data_dict, kernel_size, note, batchnorm_pre, batchnorm_post, act_fn)\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'debug/feature_approximation/k_bl_ksize3_nonresidual/note{note}/kernel_size{kernel_size}/act_fn{act_fn}/batchnorm_pre{batchnorm_pre}/batchnorm_post{batchnorm_post}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0mshow_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                     model_seed=0, return_model=False)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training_extra/feature_approximation/training.py\u001b[0m in \u001b[0;36mtrain_one\u001b[0;34m(arch_json_partial, opt_config_partial, datasets, key, show_every, model_seed, train_seed, max_epoch, device, return_model, batch_size, num_phase)\u001b[0m\n\u001b[1;32m     52\u001b[0m         extra_params={\n\u001b[1;32m     53\u001b[0m             \u001b[0;34m'datasets'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'y_dim'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;34m'training_extra_config'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'num_phase'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnum_phase\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         },\n\u001b[1;32m     56\u001b[0m     )\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training_extra/training.py\u001b[0m in \u001b[0;36mtrain_one_wrapper\u001b[0;34m(get_json_fn, initialize_model_fn, get_optimizer_fn, get_loss_fn, datasets, key, show_every, model_seed, train_seed, max_epoch, early_stopping_field, device, val_test_every, return_model, extra_params, print_model)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mextra_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     )\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training_extra/training.py\u001b[0m in \u001b[0;36mtrain_one_inner\u001b[0;34m(model, datasets, key, optimizer, loss_fn, config_extra, seed, config, eval_fn, return_model, extra_params, shuffle_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m                             \u001b[0meval_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                             legacy_random_seed=True)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training_aux.py\u001b[0m in \u001b[0;36mtraining_wrapper\u001b[0;34m(model, loss_fn, optimizer, dataset_train, dataset_val, dataset_test, config, config_extra, eval_fn, key, return_model, deterministic, legacy_random_seed)\u001b[0m\n\u001b[1;32m    190\u001b[0m                            \u001b[0meval_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_normed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                            \u001b[0mf_best\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_best_tmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                            legacy_random_seed=legacy_random_seed)\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;31m# then copy/move that file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_best_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_func, optimizer, config, f_best, dataset_train, dataset_val, dataset_test, eval_fn, legacy_random_seed)\u001b[0m\n\u001b[1;32m    123\u001b[0m                                                        \u001b[0mstats_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                                                        \u001b[0mf_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_phase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                                                        legacy_random_seed)\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mstats_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats_this_phase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'========end phase {i_phase + 1}/{num_phase}=========='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training.py\u001b[0m in \u001b[0;36mtrain_one_phase\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                             \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                                             \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                                             dataset_train, print_flag)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         stats_this_batch['val'] = _val_test_one_epoch(i_epoch, model,\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training.py\u001b[0m in \u001b[0;36m_train_one_epoch\u001b[0;34m(i_epoch, model, optimizer, loss_func, conf, dataset_train, print_flag)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_minibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_this_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;31m# double check that I'm training properly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# in other words, I'm not doing dropout improperly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training_aux.py\u001b[0m in \u001b[0;36mgenerate_new_n_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_new_n_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/my_data/thesis-yimeng-v2/thesis_v2/training/training_aux.py\u001b[0m in \u001b[0;36mcycle_reboot\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# cycle('ABCD') --> A B C D A B C D A B C D ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "handle_one_case(\n",
    "    data_dict=data_returned,\n",
    "    kernel_size=9,\n",
    "    note='debug13',\n",
    ")\n",
    "\n",
    "# 5000 max epoch should be sufficient.\n",
    "# given that this is the biggest model to train (32 out_channel) and other models take less time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
