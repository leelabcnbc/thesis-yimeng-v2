{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook sees how to create a feedforward approximator for recurrent features extracted in `scripts/feature_extraction/yuanyuan_8k_a/maskcnn_polished_with_rcnn_k_bl/20200218.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common libs\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from sys import path\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "from thesis_v2 import dir_dict\n",
    "\n",
    "from torchnetjson.builder import build_net\n",
    "from thesis_v2.training.training_aux import load_training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from that file.\n",
    "from thesis_v2.models.maskcnn_polished_with_rcnn_k_bl.builder import load_modules\n",
    "\n",
    "global_vars = {\n",
    "    'feature_file_dir': join(dir_dict['features'],\n",
    "                             'maskcnn_polished_with_rcnn_k_bl',\n",
    "                             '20200218'),\n",
    "    'augment_config': {\n",
    "        'module_names': ['layer0', 'layer1', 'layer2'],\n",
    "        'name_mapping': {\n",
    "            'moduledict.bl_stack.input_capture': 'layer0',\n",
    "            'moduledict.bl_stack.capture_list.0': 'layer1',\n",
    "            'moduledict.bl_stack.capture_list.1': 'layer2',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "from thesis_v2.configs.model.maskcnn_polished_with_rcnn_k_bl import (\n",
    "    explored_models_20200218,\n",
    "    script_keygen,\n",
    "    keygen\n",
    ")\n",
    "\n",
    "load_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_file_dir': '/my_data/thesis-yimeng-v2/results/features/maskcnn_polished_with_rcnn_k_bl/20200218', 'augment_config': {'module_names': ['layer0', 'layer1', 'layer2'], 'name_mapping': {'moduledict.bl_stack.input_capture': 'layer0', 'moduledict.bl_stack.capture_list.0': 'layer1', 'moduledict.bl_stack.capture_list.1': 'layer2'}}}\n"
     ]
    }
   ],
   "source": [
    "print(global_vars)\n",
    "\n",
    "# utils\n",
    "def get_layer_idx(friendly_name):\n",
    "    return global_vars['augment_config']['module_names'].index(friendly_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_model_param(param):\n",
    "    # test maximal number of channel in case of out of memory.\n",
    "    return param['rcnn_bl_cls'] == 4 and param['kernel_size_l23'] == 3 and param['num_layer'] == 3 and param['out_channel'] == 32\n",
    "\n",
    "\n",
    "def get_all_model_params():\n",
    "    all_params_dict = dict()\n",
    "    for idx, param in enumerate(explored_models_20200218().generate()):\n",
    "        # let's use a fully recurrent one for debugging.\n",
    "        if not good_model_param(param):\n",
    "            continue\n",
    "\n",
    "        key = keygen(**{k: v for k, v in param.items() if k not in {'scale', 'smoothness'}})\n",
    "        key_script = script_keygen(**param)\n",
    "        all_params_dict[key_script] = {\n",
    "            'key': key,\n",
    "            'param': param,\n",
    "        }\n",
    "\n",
    "    return all_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all model params\n",
    "all_params_dict = get_all_model_params()\n",
    "\n",
    "model_to_check = 's_selegacy+in_sz50+out_ch32+num_l3+k_l19+k_p3+ptavg+bn_a_fcFalse+actrelu+r_c4+r_psize1+r_ptypeNone+r_acccummean+ff1st_True+ff1stbba_True+sc0.01+sm0.000005+lmse+m_se0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract input 'topdown.0' as well as all 'bottomup.*' except the last one.\n",
    "\n",
    "def fetch_data(feature_file, grp_name):\n",
    "    slice_to_check = slice(None)\n",
    "    with h5py.File(feature_file, 'r') as f_feature:\n",
    "        grp = f_feature[grp_name]\n",
    "        num_bottom_up = len([x for x in grp if x.startswith(str(get_layer_idx('layer2')) + '.')])\n",
    "        assert num_bottom_up > 1\n",
    "        assert num_bottom_up == len([x for x in grp if x.startswith(str(get_layer_idx('layer1')) + '.')])\n",
    "        \n",
    "        pcn_in = grp[str(get_layer_idx('layer0')) + '.0'][slice_to_check]\n",
    "        pcn_inter_list = [grp[str(get_layer_idx('layer1')) + f'.{x}'][slice_to_check] for x in range(num_bottom_up)]\n",
    "        pcn_out_list = [grp[str(get_layer_idx('layer2')) + f'.{x}'][slice_to_check] for x in range(num_bottom_up)]\n",
    "    \n",
    "    print((pcn_in.shape, pcn_in.mean(), pcn_in.std(), pcn_in.min(), pcn_in.max()))\n",
    "    print([(x.shape, x.mean(), x.std(), x.min(), x.max()) for x in pcn_inter_list])\n",
    "    print([(x.shape, x.mean(), x.std(), x.min(), x.max()) for x in pcn_out_list])\n",
    "    \n",
    "    return {\n",
    "        'in': pcn_in,\n",
    "        'inter_list': pcn_inter_list,\n",
    "        'out_list': pcn_out_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((5120, 32, 42, 42), 0.00097621273, 0.13129921, -1.5368805, 1.6458784)\n",
      "[((5120, 32, 42, 42), 0.30902508, 0.5585249, 0.0, 13.410167), ((5120, 32, 42, 42), 0.38653353, 0.5509103, 0.0, 10.5434675), ((5120, 32, 42, 42), 0.38611388, 0.5902634, 0.0, 10.2964525), ((5120, 32, 42, 42), 0.38038015, 0.58427274, 0.0, 11.386213)]\n",
      "[((5120, 32, 42, 42), 0.8971239, 1.1501049, 0.0, 28.444695), ((5120, 32, 42, 42), 0.9062065, 1.3005654, 0.0, 23.221088), ((5120, 32, 42, 42), 0.87318933, 1.303688, 0.0, 23.964273), ((5120, 32, 42, 42), 0.8348414, 1.2768589, 0.0, 22.642118)]\n"
     ]
    }
   ],
   "source": [
    "data_returned = fetch_data(join(global_vars['feature_file_dir'], model_to_check + '.hdf5'), 'X_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "def load_model(key):\n",
    "    result = load_training_results(key, return_model=False)\n",
    "    # load twice, first time to get the model.\n",
    "    model_ = load_training_results(key, return_model=True, model=build_net(result['config_extra']['model']))['model']\n",
    "    model_.cuda()\n",
    "    model_.eval()\n",
    "    return model_\n",
    "\n",
    "model = load_model(all_params_dict[model_to_check]['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "-0.006521218 0.6765042 -11.364738 8.701018\n",
      "(32, 32, 42, 42) (32, 32, 42, 42) (32, 32, 42, 42)\n",
      "0.0\n",
      "0.0\n",
      "(0, 2)\n",
      "-0.03389127 0.8185497 -13.730714 9.92229\n",
      "(32, 32, 42, 42) (32, 32, 42, 42) (32, 32, 42, 42)\n",
      "0.0\n",
      "0.0\n",
      "(0, 3)\n",
      "-0.06841364 0.91917163 -15.834566 10.484059\n",
      "(32, 32, 42, 42) (32, 32, 42, 42) (32, 32, 42, 42)\n",
      "0.0\n",
      "0.0\n",
      "(1, 2)\n",
      "-0.027370047 0.47159806 -7.076529 5.2973967\n",
      "(32, 32, 42, 42) (32, 32, 42, 42) (32, 32, 42, 42)\n",
      "0.0\n",
      "0.0\n",
      "(1, 3)\n",
      "-0.061892413 0.6678318 -9.848972 7.0623784\n",
      "(32, 32, 42, 42) (32, 32, 42, 42) (32, 32, 42, 42)\n",
      "0.0\n",
      "0.0\n",
      "(2, 3)\n",
      "-0.034522373 0.39061153 -4.7467127 4.2083216\n",
      "(32, 32, 42, 42) (32, 32, 42, 42) (32, 32, 42, 42)\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# # the idea is, given idx1 and idx2, predict out_list[idx2] - out_list[idx1]  given (out_list[idx1]  and in).\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from torch.backends import cudnn\n",
    "import torch\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "def check_similarity(d1, d2):\n",
    "    assert d1.shape == d2.shape\n",
    "    norm_diff = norm(d1-d2)/norm(d2)\n",
    "    print(norm_diff)\n",
    "    print(abs(d1-d2).max())\n",
    "    assert norm_diff < 1e-5\n",
    "\n",
    "def debug_result(model_,\n",
    "                 in_,\n",
    "                 inter_,\n",
    "                 out_,\n",
    "                 idx_diff,\n",
    "                 # this determines which BN layer to use.\n",
    "                 time_start,\n",
    "                ):\n",
    "    assert idx_diff > 0\n",
    "    assert time_start >= 0\n",
    "    \n",
    "    print(in_.shape, inter_.shape, out_.shape)\n",
    "    \n",
    "    model_ = model_.moduledict['bl_stack']\n",
    "    \n",
    "    assert model_.n_layer == 2\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        last_out = [torch.tensor(inter_).cuda(), torch.tensor(out_).cuda()]\n",
    "        b_input = torch.tensor(in_).cuda()\n",
    "        for t in range(time_start, time_start + idx_diff):\n",
    "            for layer_idx in range(model_.n_layer):\n",
    "                layer_this = model_.layer_list[layer_idx]\n",
    "                bn_this = model_.bn_layer_list[t * model_.n_layer + layer_idx]\n",
    "                if layer_idx == 0:\n",
    "                    last_out[layer_idx] = layer_this(b_input, last_out[layer_idx])\n",
    "                else:\n",
    "                    last_out[layer_idx] = layer_this(last_out[layer_idx - 1], last_out[layer_idx])\n",
    "                # do batch norm\n",
    "                last_out[layer_idx] = bn_this(last_out[layer_idx])\n",
    "                # do act\n",
    "                last_out[layer_idx] = model_.act_fn(last_out[layer_idx])\n",
    "    return last_out[-1].cpu().numpy() - out_\n",
    "\n",
    "def check_result(model_, data_dict):\n",
    "    num_out = len(data_dict['out_list'])\n",
    "    \n",
    "    # only start from the beginning. otherwise we need some buffer to calculate.\n",
    "    for idx1 in range(num_out):\n",
    "        for idx2 in range(idx1+1, num_out):\n",
    "            print((idx1, idx2))\n",
    "            result_ref = data_dict['out_list'][idx2][:32] - data_dict['out_list'][idx1][:32]\n",
    "            print(result_ref.mean(), result_ref.std(), result_ref.min(), result_ref.max())\n",
    "            result_debug = debug_result(model_,data_dict['in'][:32],\n",
    "                                        data_dict['inter_list'][idx1][:32],\n",
    "                                        data_dict['out_list'][idx1][:32],\n",
    "                                        idx2-idx1,\n",
    "                                        idx1+1)\n",
    "            check_similarity(result_ref, result_debug)\n",
    "\n",
    "# all ok.\n",
    "check_result(model, data_returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now time to get a model to train it.\n",
    "# simple stuff. conv + relu.\n",
    "# maybe with BN.\n",
    "\n",
    "# two kinds of models\n",
    "\n",
    "# BN + conv + ReLU + BN\n",
    "# conv + ReLU + BN\n",
    "# I may want to constrain the first BN a bit,\n",
    "# say, all in_ channels share the same scale and bias; same goes with out1 channels.\n",
    "\n",
    "# some concerns: stats are different for `out_` at different iterations.\n",
    "# but let's ignore it for now.\n",
    "from thesis_v2.models.feature_approximation.builder import (\n",
    "    gen_local_pcn_recurrent_feature_approximator\n",
    ")\n",
    "\n",
    "from thesis_v2.training_extra.feature_approximation.opt import get_feature_approximation_opt_config\n",
    "from thesis_v2.training_extra.feature_approximation.training import train_one\n",
    "\n",
    "def handle_one_case(*,\n",
    "                    data_dict,\n",
    "                    kernel_size,\n",
    "                    note,\n",
    "                    batchnorm_pre=True,\n",
    "                    batchnorm_post=True,\n",
    "                    act_fn='relu',\n",
    "                   ):\n",
    "    \n",
    "    # prepare dataset\n",
    "    num_out = len(data_dict['out_list'])\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for idx1 in range(0, num_out-1):\n",
    "        # we should use a fixed idx1, as the function to be modeled depends not only on idx1,\n",
    "        # but also on idx2 - idx1.\n",
    "        for idx2 in range(idx1+1, num_out):\n",
    "            if idx2 - idx1 != 2:\n",
    "                continue\n",
    "            print((idx1, idx2))\n",
    "            x_train.append(np.concatenate([data_dict['in'],data_dict['inter_list'][idx1],data_dict['out_list'][idx1]], axis=1))\n",
    "            # using the difference (`data_dict['out_list'][idx2] - data_dict['out_list'][idx1]`)\n",
    "            #    or data_dict['out_list'][idx2] makes little difference.\n",
    "            y_train.append(data_dict['out_list'][idx2] - data_dict['out_list'][idx1])\n",
    "        break\n",
    "    \n",
    "    x_train = np.concatenate(x_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    \n",
    "    print((x_train.shape, y_train.shape))\n",
    "    print(y_train.mean(), y_train.std())\n",
    "    \n",
    "    dataset_this = {\n",
    "        'X_train': x_train,\n",
    "        'y_train': y_train,\n",
    "    }\n",
    "    \n",
    "    def gen_cnn_partial(in_shape, in_y_shape):\n",
    "        assert len(in_shape) == 3\n",
    "        assert len(in_y_shape) == 3\n",
    "        in_higher_c = in_y_shape[0]\n",
    "        in_lower_c = in_shape[0] - in_higher_c\n",
    "        \n",
    "        return gen_local_pcn_recurrent_feature_approximator(\n",
    "            in_shape_lower=[in_lower_c, in_shape[1], in_shape[2]],\n",
    "            in_shape_higher=[in_higher_c, in_shape[1], in_shape[2]],\n",
    "            kernel_size=kernel_size,\n",
    "            act_fn=act_fn,\n",
    "        )\n",
    "    #\n",
    "    res = train_one(arch_json_partial=gen_cnn_partial,\n",
    "                    opt_config_partial=get_feature_approximation_opt_config,\n",
    "                    datasets=dataset_this,\n",
    "                    # note this gets saved under v1 folder...\n",
    "                    # but it should not matter.\n",
    "                    key=f'debug/feature_approximation/k_bl_ksize3/note{note}/kernel_size{kernel_size}/act_fn{act_fn}/batchnorm_pre{batchnorm_pre}/batchnorm_post{batchnorm_post}',\n",
    "                    show_every=50,\n",
    "                    model_seed=0, return_model=False)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2)\n",
      "((5120, 96, 42, 42), (5120, 32, 42, 42))\n",
      "-0.023933753 0.85231274\n",
      "num_param 249120\n",
      "num of phase:  3\n",
      "========starting phase 1/3==========\n",
      "========starting epoch 0==========\n",
      "0-0, train loss 1.6959259510040283\n",
      "train loss 1.6959259510040283\n",
      "========done epoch 0==========\n",
      "========starting epoch 50==========\n",
      "50-0, train loss 0.6478598713874817\n",
      "train loss 0.6478598713874817\n",
      "========done epoch 50==========\n",
      "========starting epoch 100==========\n",
      "100-0, train loss 0.4719250798225403\n",
      "train loss 0.4719250798225403\n",
      "========done epoch 100==========\n",
      "========starting epoch 150==========\n",
      "150-0, train loss 0.38665175437927246\n",
      "train loss 0.38665175437927246\n",
      "========done epoch 150==========\n",
      "========starting epoch 200==========\n",
      "200-0, train loss 0.3336195945739746\n",
      "train loss 0.3336195945739746\n",
      "========done epoch 200==========\n",
      "========starting epoch 250==========\n",
      "250-0, train loss 0.30563950538635254\n",
      "train loss 0.30563950538635254\n",
      "========done epoch 250==========\n",
      "========starting epoch 300==========\n",
      "300-0, train loss 0.2934873700141907\n",
      "train loss 0.2934873700141907\n",
      "========done epoch 300==========\n",
      "========starting epoch 350==========\n",
      "350-0, train loss 0.2785651385784149\n",
      "train loss 0.2785651385784149\n",
      "========done epoch 350==========\n",
      "========starting epoch 400==========\n",
      "400-0, train loss 0.26933735609054565\n",
      "train loss 0.26933735609054565\n",
      "========done epoch 400==========\n",
      "========starting epoch 450==========\n",
      "450-0, train loss 0.26451021432876587\n",
      "train loss 0.26451021432876587\n",
      "========done epoch 450==========\n",
      "========starting epoch 500==========\n",
      "500-0, train loss 0.2551293969154358\n",
      "train loss 0.2551293969154358\n",
      "========done epoch 500==========\n",
      "========starting epoch 550==========\n",
      "550-0, train loss 0.2604365646839142\n",
      "train loss 0.2604365646839142\n",
      "========done epoch 550==========\n",
      "========starting epoch 600==========\n",
      "600-0, train loss 0.24625247716903687\n",
      "train loss 0.24625247716903687\n",
      "========done epoch 600==========\n",
      "========starting epoch 650==========\n",
      "650-0, train loss 0.24323692917823792\n",
      "train loss 0.24323692917823792\n",
      "========done epoch 650==========\n",
      "========starting epoch 700==========\n",
      "700-0, train loss 0.24547043442726135\n",
      "train loss 0.24547043442726135\n",
      "========done epoch 700==========\n",
      "========starting epoch 750==========\n",
      "750-0, train loss 0.23714926838874817\n",
      "train loss 0.23714926838874817\n",
      "========done epoch 750==========\n",
      "========starting epoch 800==========\n",
      "800-0, train loss 0.2401343584060669\n",
      "train loss 0.2401343584060669\n",
      "========done epoch 800==========\n",
      "========starting epoch 850==========\n",
      "850-0, train loss 0.2358197122812271\n",
      "train loss 0.2358197122812271\n",
      "========done epoch 850==========\n",
      "========starting epoch 900==========\n",
      "900-0, train loss 0.23689348995685577\n",
      "train loss 0.23689348995685577\n",
      "========done epoch 900==========\n",
      "========starting epoch 950==========\n",
      "950-0, train loss 0.22968816757202148\n",
      "train loss 0.22968816757202148\n",
      "========done epoch 950==========\n",
      "========starting epoch 1000==========\n",
      "1000-0, train loss 0.22934776544570923\n",
      "train loss 0.22934776544570923\n",
      "========done epoch 1000==========\n",
      "========starting epoch 1050==========\n",
      "1050-0, train loss 0.2338610142469406\n",
      "train loss 0.2338610142469406\n",
      "========done epoch 1050==========\n",
      "========starting epoch 1100==========\n",
      "1100-0, train loss 0.23186732828617096\n",
      "train loss 0.23186732828617096\n",
      "========done epoch 1100==========\n",
      "========starting epoch 1150==========\n",
      "1150-0, train loss 0.2362423986196518\n",
      "train loss 0.2362423986196518\n",
      "========done epoch 1150==========\n",
      "========starting epoch 1200==========\n",
      "1200-0, train loss 0.23041456937789917\n",
      "train loss 0.23041456937789917\n",
      "========done epoch 1200==========\n",
      "========starting epoch 1250==========\n",
      "1250-0, train loss 0.2293604463338852\n",
      "train loss 0.2293604463338852\n",
      "========done epoch 1250==========\n",
      "========starting epoch 1300==========\n",
      "1300-0, train loss 0.2319389283657074\n",
      "train loss 0.2319389283657074\n",
      "========done epoch 1300==========\n",
      "========starting epoch 1350==========\n",
      "1350-0, train loss 0.22708728909492493\n",
      "train loss 0.22708728909492493\n",
      "========done epoch 1350==========\n",
      "========starting epoch 1400==========\n",
      "1400-0, train loss 0.2271415740251541\n",
      "train loss 0.2271415740251541\n",
      "========done epoch 1400==========\n",
      "========starting epoch 1450==========\n",
      "1450-0, train loss 0.22534437477588654\n",
      "train loss 0.22534437477588654\n",
      "========done epoch 1450==========\n",
      "========starting epoch 1500==========\n",
      "1500-0, train loss 0.22697769105434418\n",
      "train loss 0.22697769105434418\n",
      "========done epoch 1500==========\n",
      "========starting epoch 1550==========\n",
      "1550-0, train loss 0.22612357139587402\n",
      "train loss 0.22612357139587402\n",
      "========done epoch 1550==========\n",
      "========starting epoch 1600==========\n",
      "1600-0, train loss 0.2270386517047882\n",
      "train loss 0.2270386517047882\n",
      "========done epoch 1600==========\n",
      "========starting epoch 1650==========\n",
      "1650-0, train loss 0.2293662577867508\n",
      "train loss 0.2293662577867508\n",
      "========done epoch 1650==========\n",
      "========starting epoch 1700==========\n",
      "1700-0, train loss 0.23326021432876587\n",
      "train loss 0.23326021432876587\n",
      "========done epoch 1700==========\n",
      "========starting epoch 1750==========\n",
      "1750-0, train loss 0.23083344101905823\n",
      "train loss 0.23083344101905823\n",
      "========done epoch 1750==========\n",
      "========starting epoch 1800==========\n",
      "1800-0, train loss 0.2305261492729187\n",
      "train loss 0.2305261492729187\n",
      "========done epoch 1800==========\n",
      "========starting epoch 1850==========\n",
      "1850-0, train loss 0.22604301571846008\n",
      "train loss 0.22604301571846008\n",
      "========done epoch 1850==========\n",
      "========starting epoch 1900==========\n",
      "1900-0, train loss 0.22968368232250214\n",
      "train loss 0.22968368232250214\n",
      "========done epoch 1900==========\n",
      "========starting epoch 1950==========\n",
      "1950-0, train loss 0.23124277591705322\n",
      "train loss 0.23124277591705322\n",
      "========done epoch 1950==========\n",
      "========starting epoch 2000==========\n",
      "2000-0, train loss 0.22352096438407898\n",
      "train loss 0.22352096438407898\n",
      "========done epoch 2000==========\n",
      "========starting epoch 2050==========\n",
      "2050-0, train loss 0.23170194029808044\n",
      "train loss 0.23170194029808044\n",
      "========done epoch 2050==========\n",
      "========starting epoch 2100==========\n",
      "2100-0, train loss 0.22299087047576904\n",
      "train loss 0.22299087047576904\n",
      "========done epoch 2100==========\n",
      "========starting epoch 2150==========\n",
      "2150-0, train loss 0.22645947337150574\n",
      "train loss 0.22645947337150574\n",
      "========done epoch 2150==========\n",
      "========starting epoch 2200==========\n",
      "2200-0, train loss 0.22852873802185059\n",
      "train loss 0.22852873802185059\n",
      "========done epoch 2200==========\n",
      "========starting epoch 2250==========\n",
      "2250-0, train loss 0.22563442587852478\n",
      "train loss 0.22563442587852478\n",
      "========done epoch 2250==========\n",
      "========starting epoch 2300==========\n",
      "2300-0, train loss 0.2294737994670868\n",
      "train loss 0.2294737994670868\n",
      "========done epoch 2300==========\n",
      "========starting epoch 2350==========\n",
      "2350-0, train loss 0.22507043182849884\n",
      "train loss 0.22507043182849884\n",
      "========done epoch 2350==========\n",
      "========starting epoch 2400==========\n",
      "2400-0, train loss 0.2254313975572586\n",
      "train loss 0.2254313975572586\n",
      "========done epoch 2400==========\n",
      "========starting epoch 2450==========\n",
      "2450-0, train loss 0.2308899611234665\n",
      "train loss 0.2308899611234665\n",
      "========done epoch 2450==========\n",
      "========starting epoch 2500==========\n",
      "2500-0, train loss 0.22387509047985077\n",
      "train loss 0.22387509047985077\n",
      "========done epoch 2500==========\n",
      "========starting epoch 2550==========\n",
      "2550-0, train loss 0.23052991926670074\n",
      "train loss 0.23052991926670074\n",
      "========done epoch 2550==========\n",
      "========starting epoch 2600==========\n",
      "2600-0, train loss 0.22643713653087616\n",
      "train loss 0.22643713653087616\n",
      "========done epoch 2600==========\n",
      "========starting epoch 2650==========\n",
      "2650-0, train loss 0.22454772889614105\n",
      "train loss 0.22454772889614105\n",
      "========done epoch 2650==========\n",
      "========starting epoch 2700==========\n",
      "2700-0, train loss 0.22488023340702057\n",
      "train loss 0.22488023340702057\n",
      "========done epoch 2700==========\n",
      "========starting epoch 2750==========\n",
      "2750-0, train loss 0.22429613769054413\n",
      "train loss 0.22429613769054413\n",
      "========done epoch 2750==========\n",
      "========starting epoch 2800==========\n",
      "2800-0, train loss 0.2225354015827179\n",
      "train loss 0.2225354015827179\n",
      "========done epoch 2800==========\n",
      "========starting epoch 2850==========\n",
      "2850-0, train loss 0.22780875861644745\n",
      "train loss 0.22780875861644745\n",
      "========done epoch 2850==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========starting epoch 2900==========\n",
      "2900-0, train loss 0.22126905620098114\n",
      "train loss 0.22126905620098114\n",
      "========done epoch 2900==========\n",
      "========starting epoch 2950==========\n",
      "2950-0, train loss 0.2183196246623993\n",
      "train loss 0.2183196246623993\n",
      "========done epoch 2950==========\n",
      "========starting epoch 3000==========\n",
      "3000-0, train loss 0.22487185895442963\n",
      "train loss 0.22487185895442963\n",
      "========done epoch 3000==========\n",
      "========starting epoch 3050==========\n",
      "3050-0, train loss 0.2210100293159485\n",
      "train loss 0.2210100293159485\n",
      "========done epoch 3050==========\n",
      "========starting epoch 3100==========\n",
      "3100-0, train loss 0.22461624443531036\n",
      "train loss 0.22461624443531036\n",
      "========done epoch 3100==========\n",
      "========starting epoch 3150==========\n",
      "3150-0, train loss 0.22682519257068634\n",
      "train loss 0.22682519257068634\n",
      "========done epoch 3150==========\n",
      "========starting epoch 3200==========\n",
      "3200-0, train loss 0.2230764776468277\n",
      "train loss 0.2230764776468277\n",
      "========done epoch 3200==========\n",
      "========starting epoch 3250==========\n",
      "3250-0, train loss 0.22390270233154297\n",
      "train loss 0.22390270233154297\n",
      "========done epoch 3250==========\n",
      "========starting epoch 3300==========\n",
      "3300-0, train loss 0.22248750925064087\n",
      "train loss 0.22248750925064087\n",
      "========done epoch 3300==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-add7dfba14a4>\", line 4, in <module>\n",
      "    note='debug13',\n",
      "  File \"<ipython-input-10-fd7390de6e9a>\", line 80, in handle_one_case\n",
      "    model_seed=0, return_model=False)\n",
      "  File \"/my_data/thesis-yimeng-v2/thesis_v2/training_extra/feature_approximation/training.py\", line 54, in train_one\n",
      "    'training_extra_config': {'num_phase': num_phase},\n",
      "  File \"/my_data/thesis-yimeng-v2/thesis_v2/training_extra/training.py\", line 133, in train_one_wrapper\n",
      "    extra_params=extra_params,\n",
      "  File \"/my_data/thesis-yimeng-v2/thesis_v2/training_extra/training.py\", line 54, in train_one_inner\n",
      "    legacy_random_seed=True)\n",
      "  File \"/my_data/thesis-yimeng-v2/thesis_v2/training/training_aux.py\", line 192, in training_wrapper\n",
      "    legacy_random_seed=legacy_random_seed)\n",
      "  File \"/my_data/thesis-yimeng-v2/thesis_v2/training/training.py\", line 125, in train\n",
      "    legacy_random_seed)\n",
      "  File \"/my_data/thesis-yimeng-v2/thesis_v2/training/training.py\", line 350, in train_one_phase\n",
      "    dataset_train, print_flag)\n",
      "  File \"/my_data/thesis-yimeng-v2/thesis_v2/training/training.py\", line 250, in _train_one_epoch\n",
      "    for i_minibatch, data_this_batch in enumerate(dataset):\n",
      "  File \"/my_data/thesis-yimeng-v2/thesis_v2/training/training_aux.py\", line 47, in generate_new_n_batch\n",
      "    next_batch = next(loader)\n",
      "  File \"/my_data/thesis-yimeng-v2/thesis_v2/training/training_aux.py\", line 31, in cycle_reboot\n",
      "    for element in iterable:\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 346, in __next__\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 79, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 79, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/opt/conda/envs/leelab/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "handle_one_case(\n",
    "    data_dict=data_returned,\n",
    "    kernel_size=9,\n",
    "    note='debug13',\n",
    ")\n",
    "\n",
    "# 5000 max epoch should be sufficient.\n",
    "# given that this is the biggest model to train (32 out_channel) and other models take less time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
